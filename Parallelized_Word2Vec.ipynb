{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usefull packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import copy\n",
    "import time\n",
    "from matplotlib.pyplot import plot\n",
    "\n",
    "#pycuda\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Element Logiciel Pour le Traitement de Donnees Massives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autour de l'article, présentation du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "L'idée du projet est de paralléliser l'algorithme word2vec. On s'appuie pour cela sur l'article de Ji, Shihao, et al. <i>Parallelizing word2vec in shared and distributed memory</i>. Nous avons utilisé <i> pycuda </i> pour effectuer la programmation GPU. Dans ce notebook on utilise donc le langage Python comme langage d'environnement : on s'en sert principalement pour préparer les données, présenter les résultats et exécuter les fonctions à paralléliser. L'algorithme d'entraînement du réseau de neurones word2vec est en revanche codé en C.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "L'algorithme word2vec permet de représenter les mots d'un texte dans un espace de petite dimension (typiquement R<sup>d</sup> avec d de l'ordre de 10). Il permet donc de passer d'une représentation des données sous forme one-hot (où la taille de l'espace des données est égale à la taille du vocabulaire du corpus de texte étudié) à une représentaion dans R<sup>d</sup>. L'idée est que, des mots proches sémantiquement (on les suppose proches si ils sont proches dans le texte) doivent être proches dans l'espace de projection R<sup>d</sup> (au sens euclidien cette fois).\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "L'architecture de word2vec est un réseau de neurones à une couche cachée. Si l'on souhaite que notre espace de projection (celui dans lequel on souhaite représenter les mots) soit R<sup>d</sup>, on choisira d neurones dans la couche cachée. Les couches d'entrée et de sortie sont de taille V où V est la taille du vocabulaire du corpus de texte étudié. Pour construire notre matrice de projection dans R<sup>d</sup>, on entraîne ce réseau de neurones via une tâche de prédiction annexe. Pour un mot en entrée, le réseau doit prédire le où les mots les plus proches sémantiquement. La matrice des poids de passage de la couche d'input à la couche cachée sera alors notre matrice de projection. On construit donc un ensemble d'entraînement (X,Y) où on fixe la taille N du contexte utilisé et les couples (Xi,Yi) sont determinés de la manière suivante : pour un mot Xi du texte et pour les mots Yi1, Yi2,...,YiN qui sont autour du mot Xi dans le texte les couples (Xi,Yi1),...(Xi,YiN) forment la base d'entraînement.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "L'entraînement de ce réseau nécessite l'implémentation de l'algorithme de descente de gradient. L'article met en avant les inconvénients de cet algorithme dans ce cas précis, en particulier l'évaluation de la fonction de perte et de ses gradients est très gourmande (le nombre de calculs étant proportionnel au nombre de mots dans le texte). On préférera donc utiliser la méthode de descente de gradient stochastique (avec negative sampling) qui n'utilise qu'une faible proportion des mots du texte à chaque évaluation des oracles. A chaque étape la méthode de gradient stochastique met à jour les poids correspondant à un mot Xi et à un mot de son contexte Yij. Le negative sampling (qui permet d'améliorer la convergence de l'algorithme) consiste à prendre des mots au hasard dans le texte à chaque étape et à mettre à jour les poids correspondant en considérant que ces mots ne sont pas dans le contexte de Xi.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "Dans l'article, les auteurs proposent, pour accélerer l'implémentation de la descente de gradient stochastique, de paralléliser les boucles de mise à jour des poids du réseau (voir section III de l'article). Cela risque de créer des conflits (puisque les mêmes poids risquent d'être mis à jour au même moment) mais on s'en affranchit, cela peut réduire théoriquement les taux de convergence de l'algorithme mais permet de gagner en vitesse (d'un facteur du nombre de parallélisation). Nous n'utilisons donc aucun verrou et les conflits sont donc possibles. On observe que pour des grands textes et à condition de ne pas utiliser un nombre de threads trop élevé, les conflits sont rares et n'empechent pas la convergence. L'algorihme que l'on parallélise est l'implémentation d'Hogwild (voir <i>Algorithm 1</i> de l'article) qui correspond simplement à une étape de la descente de gradient stochastique.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "La deuxième amélioration proposée dans l'article est double. Elle consiste à exploiter plus encore les propriétés locales des informations utilisées à chaque étape de la descente de gradient. L'idée est, dans un premier temps, de partager le <i>negative sampling</i> entre les mots d'un même contexte. Cela va permettre dans un second temps de passer des opérations de produits scalaires (opérations BLAS-1)  utilisés massivement dans l'implémentation d'Hogwild à des produits matriciels (opérations BLAS-3) en théorie plus efficaces en utilisant des bibliothèques performantes d'algèbre linéaire.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "On s'attache dans ce projet à mettre en place ces deux techniques sur un petit jeu de données on observera ainsi les gains de performance obtenus.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hogwild algorithm in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "On commence par coder l'algorithme sous Python. Cette première implémentation de la méthode ne sera donc pas parallélisée et nous permettra de comparer les résultats et la vitesse de calcul avec le code parallélisé sous pycuda. On rédige volontairement un code non optimisé (nous n'utilisons pas les produits matriciels de numpy par exemple) ce qui nous permettra de constater effectivement les gains de performance dûs à l'utilisation de CUDA.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    \"\"\" fonction sigmoid.\n",
    "    x : un réel (float)\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def hog_loop(Min,Mout,alpha,wout,Nwin,negative):\n",
    "    \"\"\" Une boucle permettant la mise à jour des poids du réseau à partir d'un mot et de son contexte\n",
    "    Min (array) : La matrice des poids du réseau de neurones reliant la couche d'input à la couche cachée (taille V*d)\n",
    "    Mout (array) : La matrice des poids du réseau de neurones reliant la couche cachée à la couche d'output (taille V*d)\n",
    "    alpha (float) : learning rate\n",
    "    wout (int) : un entier représentant le mot qui va être mis à jour\n",
    "    Nwin (array) : les mots du contexte de wout\n",
    "    negative (int) : taille du negative sampling\n",
    "    \n",
    "    return : Min, Mout les poids mis à jour\n",
    "    \"\"\"\n",
    "    V,d = Min.shape\n",
    "    N = Nwin.shape[0]\n",
    "    for i in range(N):\n",
    "        input_word = Nwin[i]\n",
    "        temp = np.array([0.0]*d)\n",
    "        for k in range(negative+1):\n",
    "            if k == 0:\n",
    "                target_word = wout\n",
    "                label = 1\n",
    "            else :\n",
    "                #negative sampling\n",
    "                target_word = rd.randint(0,V-1)\n",
    "                label = 0\n",
    "            inn = 0\n",
    "            for j in range(d):\n",
    "                inn = inn + Min[input_word,j]*Mout[target_word,j]\n",
    "            err = label - sig(inn)\n",
    "            for j in range(d):\n",
    "                temp[j] = temp[j] + err*Mout[target_word,j]\n",
    "            for j in range(d):\n",
    "                #MAJ Mout\n",
    "                Mout[target_word,j] = Mout[target_word,j] + alpha*err*Min[input_word,j]\n",
    "        for j in range(d):\n",
    "            #MAJ Min\n",
    "            Min[input_word,j] = Min[input_word,j] + alpha*temp[j]\n",
    "    return(Min,Mout)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(X,params):\n",
    "    \"\"\" fonction d'entraînement du reseau de neurones\n",
    "    X (array) : texte constituant la base d'entraînement\n",
    "    params (dict) :  N, n_epochs, alpha, negative, d\n",
    "    N (int) : taille du contexte d'un mot \n",
    "    n_epochs (int) : nombre d'entraînement complet du réseau\n",
    "    alpha (float) : learning rate\n",
    "    negative (int) : taille du negative sampling\n",
    "    d (int) : taille de l'espace de représentation\n",
    "    \n",
    "    return : Min, Mout (array) les matrices de poids du réseau entraîné\n",
    "    \"\"\"\n",
    "    \n",
    "    N = params['N']\n",
    "    n_epochs = params['n_epochs']\n",
    "    alpha = params['alpha']\n",
    "    negative = params['negative']\n",
    "    d = params['d']\n",
    "    \n",
    "    V = np.max(X)+1 #taille du vocabulaire\n",
    "    n_words = np.shape(X)[0] #nombre de mots du corpus\n",
    "    \n",
    "    #Initialisation du réseau\n",
    "    Min = np.array([rd.random() for i in range(V*d)]).reshape((V,d))\n",
    "    Mout = np.array([rd.random() for i in range(V*d)]).reshape((V,d))\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        epochs_order = np.array([t for t in range(N,n_words-N)])\n",
    "        np.random.shuffle(epochs_order)\n",
    "        \n",
    "        for j in epochs_order:\n",
    "            \n",
    "            wout = X[j]\n",
    "            \n",
    "            #Calcul du contexte de wout\n",
    "            Nwin = []\n",
    "            for k in range(N):\n",
    "                Nwin.append(X[j-N+k])\n",
    "            for k in range(N):\n",
    "                Nwin.append(X[j+k+1])\n",
    "            Nwin = np.array(Nwin)\n",
    "            \n",
    "            #Maj de wout et de son contexte\n",
    "            Min, Mout = hog_loop(Min,Mout,alpha,wout,Nwin,negative)\n",
    "            \n",
    "    \n",
    "    return Min,Mout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme d'Hogwild parallélisé avec pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous commençons par simplement paralléliser l'algorithme précédent (en particulier la fonction hog_loop). Les autres contributions de l'article seront developpées dans un second temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "\n",
    "#include <time.h>\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <curand.h>\n",
    "#include <math.h>\n",
    "\n",
    "__device__ void loop(float *Min, float *Mout, float alpha, int *Nwin, int wout, int V, int d, int N, int negative, int random){\n",
    "    \n",
    "    /*\n",
    "    Une boucle permettant la mise à jour des poids du réseau à partir d'un mot et de son contexte\n",
    "    \n",
    "    float *Min : La matrice des poids du réseau de neurones reliant la couche d'input à la couche cachée (taille V*d)\n",
    "    float *Mout : La matrice des poids du réseau de neurones reliant la couche cachée à la couche d'output (taille V*d)\n",
    "    float alpha : learning rate\n",
    "    int *Nwin : contexte de wout\n",
    "    int wout : mot mis à jour\n",
    "    int V : taille du vocabulaire (nombre de mots différents)\n",
    "    int d : taille de l'espace de projection\n",
    "    int N : taille du contexte utilisé pour l'apprentissage (de chaque coté donc 2*N en tout)\n",
    "    int negative : taille du negative sampling\n",
    "    int random : un entier aléatoire (base pour construire le negative sampling)\n",
    "    */\n",
    "\n",
    "    /* Initialisation des variables */\n",
    "    float* temp;\n",
    "    temp = (float *)malloc(sizeof(float)*d);\n",
    "    int label;\n",
    "    int target_word;\n",
    "    float inn;\n",
    "    float err;\n",
    "    int r;\n",
    "    \n",
    "    /* Boucle principale sur les 2*N inputs*/\n",
    "    for(int i=0;i<2*N;i++){\n",
    "        int input_word = Nwin[i];\n",
    "        for(int j=0;j<d;j++){\n",
    "            temp[j]=0;\n",
    "        }\n",
    "        for(int k=0; k<negative+1;k++){\n",
    "            if (k==0){\n",
    "                target_word = wout;\n",
    "                label = 1;\n",
    "            } else {\n",
    "                /* negative sampling  (avec un pseudo-random generator)*/\n",
    "                r = (1664525*r+1013904223) % 4294967296;\n",
    "                target_word = r%V;\n",
    "                label = 0;\n",
    "            }\n",
    "            inn = 0;\n",
    "            for(int j=0;j<d;j++){\n",
    "                inn = inn + Min[input_word*d+j]*Mout[target_word*d+j];\n",
    "            }\n",
    "            err = label-(1/(1+exp(-inn)));\n",
    "            for(int j=0;j<d;j++){\n",
    "                temp[j] = temp[j]+err*Mout[target_word*d+j];\n",
    "            }\n",
    "            for(int j=0;j<d;j++){\n",
    "                /* MAJ Mout */\n",
    "                Mout[target_word*d+j] = Mout[target_word*d+j]+alpha*err*Min[input_word*d+j];\n",
    "            }  \n",
    "        }\n",
    "        for(int j=0;j<d;j++){\n",
    "            /* MAJ Min */\n",
    "            Min[input_word*d+j] = Min[input_word*d+j]+alpha*temp[j]; \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    free(temp);\n",
    "}\n",
    "\n",
    "\n",
    "__global__ void parallel(float *Min, float* Mout, int random, int* cst_int, float* cst_float, int* targets, int* contexts) {\n",
    "\n",
    "    /* Fonction paralélisée\n",
    "    float *Min : La matrice des poids du réseau de neurones reliant la couche d'input à la couche cachée (taille V*d)\n",
    "    float *Mout : La matrice des poids du réseau de neurones reliant la couche cachée à la couche d'output (taille V*d)\n",
    "    int* random : un entier aléatoire généré hors du device (utile pour la construction du negative sampling)\n",
    "    int* cst_int : les paramètres (int) du modèle (V,d,negative,N)\n",
    "    float* cst_float : les paramètres (float) du modèle (alpha)\n",
    "    int* targets : les mots de la base d'entraînement\n",
    "    int* context : le contexte des mots de la base d'entraînement\n",
    "    */\n",
    "    \n",
    "    int V = cst_int[0];\n",
    "    int d = cst_int[1];\n",
    "    int negative = cst_int[2];\n",
    "    int N = cst_int[3];\n",
    "\n",
    "    float alpha = cst_float[0];\n",
    "    \n",
    "    /* L'index correspond au thread et indique simplement le wout sur lequel on travaille */\n",
    "    int index = threadIdx.x; \n",
    "    int wout = targets[index];\n",
    "    \n",
    "    /* Contexte de wout */\n",
    "    int *Nwin;\n",
    "    Nwin = (int *)malloc(sizeof(int)*N*2);\n",
    "    for (int i=0;i<2*N;i++){\n",
    "        Nwin[i] = contexts[index*2*N+i];\n",
    "    }\n",
    "    \n",
    "    random = random+index;\n",
    "    \n",
    "    loop(Min,Mout,alpha,Nwin,wout,V,d,N,negative,random);\n",
    "    \n",
    "    free(Nwin);\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme d'Hogwild amélioré et parallélisé avec pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "L'amélioration consiste à rendre les opérations matricielles via une astuce de mutualisation du negative sampling sur tout le contexte des mot étudiés.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_2 = SourceModule(\"\"\"\n",
    "\n",
    "\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <curand.h>\n",
    "#include <math.h>\n",
    "\n",
    "\n",
    "__device__ void multiplication(float* A, float* B, float* C, int* dim){\n",
    "    \n",
    "    /* Multplication matricielle : calcule C = A*B en supposant la multiplication licite\n",
    "    float* A\n",
    "    float* B\n",
    "    float* C \n",
    "    int* dim : contient les dimensions des matrices A et B\n",
    "    */\n",
    "    \n",
    "    int lA = dim[0];\n",
    "    int cA = dim[1];\n",
    "    int lB = dim[2];\n",
    "    int cB = dim[3];\n",
    "        \n",
    "    for(int i=0; i<lA ; i++){\n",
    "        for(int j=0; j<cB ; j++){\n",
    "            C[i*cB+j] = 0;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for(int i=0; i<lA ; i++){\n",
    "        for(int j=0; j<cB ; j++){\n",
    "            for(int k=0 ; k<cA ; k++){\n",
    "                C[i*cB+j] = C[i*cB+j] + A[i*cA+k]*B[k*cB+j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "__device__ void transpose(float* A, float* TA, int* dim){\n",
    "    /* Transposition matricielle : calcule TA = transpose(A)\n",
    "    float* A\n",
    "    float* TA\n",
    "    int* dim : contient les dimensions de la matrice A\n",
    "    */\n",
    "    \n",
    "    int lA = dim[0];\n",
    "    int cA = dim[1];\n",
    "      \n",
    "    for(int i=0; i<cA ; i++){\n",
    "        for(int j=0; j<lA ; j++){\n",
    "            TA[i*lA+j] = A[j*cA+i];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "__device__ void loop(float *Min, float *Mout, float alpha, int *Nwin, int wout, int V, int d, int N, int negative, int r){\n",
    "    /*\n",
    "    Une boucle (améliorée) permettant la mise à jour des poids du réseau à partir d'un mot et de son contexte\n",
    "    \n",
    "    float *Min : La matrice des poids du réseau de neurones reliant la couche d'input à la couche cachée (taille V*d)\n",
    "    float *Mout : La matrice des poids du réseau de neurones reliant la couche cachée à la couche d'output (taille V*d)\n",
    "    float alpha : learning rate\n",
    "    int *Nwin : contexte de wout\n",
    "    int wout : mot mis à jour\n",
    "    int V : taille du vocabulaire (nombre de mots différents)\n",
    "    int d : taille de l'espace de projection\n",
    "    int N : taille du contexte utilisé pour l'apprentissage (de chaque coté donc 2*N en tout)\n",
    "    int negative : taille du negative sampling\n",
    "    int r : un entier aléatoire (base pour construire le negative sampling)\n",
    "    */\n",
    "\n",
    "\n",
    "    /* Calcul des target_words (wout et negative sample) */\n",
    "    int *target_words;\n",
    "    target_words = (int *)malloc(sizeof(int)*(negative+1));\n",
    "    for(int i=0; i<negative+1;i++){\n",
    "        if (i==0){\n",
    "            target_words[i] = wout;\n",
    "        } else {\n",
    "            r = (1664525*r+1013904223) % 4294967296;\n",
    "            target_words[i] = r%V;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /* Matrice des labels */\n",
    "    float *label;\n",
    "    label = (float *)malloc(sizeof(float)*(2*N*(negative+1)));\n",
    "    for (int i=0; i<2*N ; i++){\n",
    "        for (int j=0 ; j<negative+1;j++){\n",
    "            if (j==0){\n",
    "                label[i*(negative+1)+j] = 1;\n",
    "            } else {\n",
    "                label[i*(negative+1)+j] = 0;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /* sub_Min les lignes de Min qui vont être utiles */\n",
    "    float* sub_Min;\n",
    "    sub_Min = (float *)malloc(sizeof(float)*2*N*d);\n",
    "    for (int i=0; i<2*N ; i++){\n",
    "        for (int j=0 ; j<d ; j++){\n",
    "            sub_Min[i*d+j] = Min[Nwin[i]*d+j];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /* sub_Mout les lignes de Mout qui vont être utiles */\n",
    "    float* sub_Mout;\n",
    "    sub_Mout = (float *)malloc(sizeof(float)*(negative+1)*d);\n",
    "    for (int i=0; i<negative+1 ; i++){\n",
    "        for (int j=0 ; j<d ; j++){\n",
    "            sub_Mout[i*d+j] = Mout[target_words[i]*d+j];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /* sub_Mout_transpose */\n",
    "    float* sub_Mout_transpose;\n",
    "    sub_Mout_transpose = (float *)malloc(sizeof(float)*d*(negative+1));\n",
    "    int dim_sub_Mout[2];\n",
    "    dim_sub_Mout[0] = negative+1;\n",
    "    dim_sub_Mout[1] = d;\n",
    "    transpose(sub_Mout, sub_Mout_transpose, dim_sub_Mout);\n",
    "    \n",
    "    /* INN */\n",
    "    float* INN;\n",
    "    INN = (float *)malloc(sizeof(float)*2*N*(negative+1));\n",
    "    int dim[4];\n",
    "    dim[0] = 2*N;\n",
    "    dim[1] = d;\n",
    "    dim[2] = d;\n",
    "    dim[3] = negative+1;\n",
    "    multiplication(sub_Min, sub_Mout_transpose, INN, dim);\n",
    "    \n",
    "    /* ERR */\n",
    "    float* ERR;\n",
    "    ERR = (float *)malloc(sizeof(float)*2*N*(negative+1));\n",
    "    for (int i=0; i<2*N ; i++){\n",
    "        for (int j=0 ; j<negative+1;j++){\n",
    "            ERR[i*(negative+1)+j] = label[i*(negative+1)+j] - (1/(1+exp(-1*INN[i*(negative+1)+j])));\n",
    "        }\n",
    "    }  \n",
    "    \n",
    "    /* TEMP */\n",
    "    float* TEMP;\n",
    "    TEMP = (float *)malloc(sizeof(float)*2*N*d);\n",
    "    dim[0] = 2*N;\n",
    "    dim[1] = negative+1;\n",
    "    dim[2] = negative+1;\n",
    "    dim[3] = d;\n",
    "    multiplication(ERR, sub_Mout, TEMP, dim);\n",
    "    \n",
    "    /* MAJ Mout */\n",
    "    float* ERR_transpose_alpha;\n",
    "    ERR_transpose_alpha = (float *)malloc(sizeof(float)*(negative+1)*2*N);\n",
    "    for (int i=0; i<negative+1 ; i++){\n",
    "        for (int j=0 ; j<2*N;j++){\n",
    "            ERR_transpose_alpha[i*2*N+j] = alpha*ERR[j*(negative+1)+i]; \n",
    "        }\n",
    "    }\n",
    "    float* MAJ_Mout;\n",
    "    MAJ_Mout = (float *)malloc(sizeof(float)*(negative+1)*d);\n",
    "    dim[0] = negative+1;\n",
    "    dim[1] = 2*N;\n",
    "    dim[2] = 2*N;\n",
    "    dim[3] = d;\n",
    "    multiplication(ERR_transpose_alpha, sub_Min, MAJ_Mout, dim);\n",
    "    for (int i=0; i<negative+1 ; i++){\n",
    "        for (int j=0 ; j<d;j++){\n",
    "            Mout[target_words[i]*d+j] = Mout[target_words[i]*d+j] + MAJ_Mout[i*d+j]; \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /* MAJ Min */\n",
    "    for(int i = 0;i<2*N;i++){\n",
    "        for(int j = 0;j<d;j++){\n",
    "            Min[Nwin[i]*d+j] = Min[Nwin[i]*d+j] + alpha*TEMP[i*d+j];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    free(target_words);\n",
    "    free(label);\n",
    "    free(sub_Min);\n",
    "    free(sub_Mout);\n",
    "    free(sub_Mout_transpose);\n",
    "    free(INN);\n",
    "    free(ERR);\n",
    "    free(TEMP);\n",
    "    free(ERR_transpose_alpha);\n",
    "    free(MAJ_Mout);\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "__global__ void parallel(float *Min, float* Mout, int random, int* cst_int, float* cst_float, int* targets, int* contexts) {\n",
    "    \n",
    "    /* Fonction paralélisée\n",
    "    float *Min : La matrice des poids du réseau de neurones reliant la couche d'input à la couche cachée (taille V*d)\n",
    "    float *Mout : La matrice des poids du réseau de neurones reliant la couche cachée à la couche d'output (taille V*d)\n",
    "    int* random : un entier aléatoire généré hors du device (utile pour la construction du negative sampling)\n",
    "    int* cst_int : les paramètres (int) du modèle (V,d,negative,N)\n",
    "    float* cst_float : les paramètres (float) du modèle (alpha)\n",
    "    int* targets : les mots de la base d'entraînement\n",
    "    int* context : le contexte des mots de la base d'entraînement\n",
    "    */\n",
    "    \n",
    "    int V = cst_int[0];\n",
    "    int d = cst_int[1];\n",
    "    int negative = cst_int[2];\n",
    "    int N = cst_int[3];\n",
    "\n",
    "    float alpha = cst_float[0];\n",
    "    \n",
    "    /*l'index correspond au thread et indique simplement le wout sur lequel on travaille*/\n",
    "    int index = threadIdx.x; \n",
    "    int wout = targets[index];\n",
    "    \n",
    "    /*Context de wout*/\n",
    "    int *Nwin;\n",
    "    Nwin = (int *)malloc(sizeof(int)*N*2);\n",
    "    for (int i=0;i<2*N;i++){\n",
    "        Nwin[i] = contexts[index*2*N+i];\n",
    "    }\n",
    "    \n",
    "    random = random+index;\n",
    "    \n",
    "    loop(Min,Mout,alpha,Nwin,wout,V,d,N,negative,random);\n",
    "   \n",
    "    free(Nwin);\n",
    "    \n",
    "}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement avec Pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante permet d'entraîner complétement le réseau de neurones via les fonctions pycuda ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec_parallel(X, params, func):\n",
    "    \"\"\" fonction d'entraînement du reseau de neurones avec pycuda\n",
    "    X (array) : texte constituant la base d'entraînement\n",
    "    params (dict) :  N, n_epochs, alpha, negative, d\n",
    "    N (int) : taille du contexte d'un mot \n",
    "    n_epochs (int) : nombre d'entraînement complet du réseau\n",
    "    alpha (float) : learning rate\n",
    "    negative (int) : taille du negative sampling\n",
    "    d (int) : taille de l'espace de représentation\n",
    "    n_parallel (int) : nombre de threads utilisés pour la parallélisation\n",
    "    func (fonction C) : la fonction à paralléliser\n",
    "    \n",
    "    return : Min, Mout (array) les matrices de poids du réseau entraîné\n",
    "    \"\"\"\n",
    "    \n",
    "    N = params['N']\n",
    "    n_epochs = params['n_epochs']\n",
    "    alpha = params['alpha']\n",
    "    negative = params['negative']\n",
    "    d = params['d']\n",
    "    n_parallel = params['n_parallel']\n",
    "    \n",
    "    V = np.max(X)+1\n",
    "    n_words = np.shape(X)[0]\n",
    "    \n",
    "    #Paramètres du modèle\n",
    "    cst_int = np.array([V,d,negative,N])\n",
    "    cst_float = np.array([alpha])\n",
    "    \n",
    "    cst_int = cst_int.astype(np.int32) #Pycuda ne peut travailler qu'avec un format 32 bits\n",
    "    cst_float = cst_float.astype(np.float32)\n",
    "    \n",
    "    #Initialisation de Min et Mout\n",
    "    Min = np.array([rd.random() for i in range(V*d)]).reshape((V,d))\n",
    "    Mout = np.array([rd.random() for i in range(V*d)]).reshape((V,d))\n",
    "    \n",
    "    Min = Min.astype(np.float32)\n",
    "    Mout = Mout.astype(np.float32)\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        epochs_order = np.array([t for t in range(N,n_words-N)])\n",
    "        np.random.shuffle(epochs_order)\n",
    "        \n",
    "        #bloc de threads\n",
    "        for j in range(n_words//n_parallel+1):\n",
    "            \n",
    "            sub_epochs_order = epochs_order[j*n_parallel:(j+1)*n_parallel]\n",
    "            \n",
    "            n_threads = np.shape(sub_epochs_order)[0]\n",
    "            \n",
    "            contexts=[]\n",
    "            targets=[]\n",
    "            \n",
    "            for k in sub_epochs_order : \n",
    "                targets.append(X[k])\n",
    "                Nwin=[]\n",
    "                for l in range(N):\n",
    "                    Nwin.append(X[k-N+l])\n",
    "                for l in range(N):\n",
    "                    Nwin.append(X[k+l+1])\n",
    "                contexts.append(Nwin)\n",
    "            \n",
    "            targets = np.array(targets)\n",
    "            contexts = np.array(contexts)\n",
    "            \n",
    "            targets = targets.astype(np.int32)\n",
    "            contexts = contexts.astype(np.int32)\n",
    "            \n",
    "            r = np.array([rd.randint(1,1000000)])\n",
    "            r = r.astype(np.int32)\n",
    "            \n",
    "            #Allocation de la mémoire sur le device\n",
    "            Min_gpu = cuda.mem_alloc(Min.nbytes)\n",
    "            Mout_gpu = cuda.mem_alloc(Mout.nbytes)\n",
    "            r_gpu = cuda.mem_alloc(r.nbytes)\n",
    "            cst_int_gpu = cuda.mem_alloc(cst_int.nbytes)\n",
    "            cst_float_gpu = cuda.mem_alloc(cst_float.nbytes)\n",
    "            targets_gpu = cuda.mem_alloc(targets.nbytes)\n",
    "            contexts_gpu = cuda.mem_alloc(contexts.nbytes)\n",
    "            \n",
    "            #Copie des variables sur le device\n",
    "            cuda.memcpy_htod(Min_gpu, Min)\n",
    "            cuda.memcpy_htod(Mout_gpu, Mout)\n",
    "            cuda.memcpy_htod(r_gpu, r)\n",
    "            cuda.memcpy_htod(cst_int_gpu,cst_int)\n",
    "            cuda.memcpy_htod(cst_float_gpu,cst_float)\n",
    "            cuda.memcpy_htod(contexts_gpu,contexts)\n",
    "            cuda.memcpy_htod(targets_gpu,targets)\n",
    "            \n",
    "            #Parallélisation\n",
    "            func(Min_gpu, Mout_gpu, r_gpu, cst_int_gpu, cst_float_gpu, targets_gpu, contexts_gpu, block=(n_threads,1,1))\n",
    "            \n",
    "            #Récupération des variables sur le host\n",
    "            cuda.memcpy_dtoh(Min, Min_gpu)\n",
    "            cuda.memcpy_dtoh(Mout, Mout_gpu)\n",
    "\n",
    "\n",
    "    return Min,Mout\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "Nous avons effectué des essais sur différents jeux de données relativement petits (de la taille d'un article ou d'un livre par exemple). Le jeu de données mentionné dans l'article nécessite de nombreuses heures d'entraînement, avec des GPU plus performants que ceux dont nous disposions et il n'était pas envisageable (ni utile) de les réutiliser à notre échelle.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "Les jeux de données (des textes brutes) doivent être traités avant de pouvoir entraîner l'algorithme word2vec. L'idée est de commencer par nettoyer les corpus en supprimant les caractères spéciaux et les majuscules. Il faut ensuite encoder les textes via la méthode one-hot. En l'occurrence, nous avons choisi de travailler avec une variante de l'encodage one-hot. Un corpus de texte est alors représenté par un vecteur X de taille n (le nombre de mots du corpus), on donne à chaque mot un identidiant et l'élement Xi est l'identifiant du ième mot du corpus.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "Les fonctions relatives au nettoyage des données ne sont pas présentées dans ce notebook car elles sont simples et ne présentent pas d'intérêt pour cet exposé.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extrait des misérables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chargement du jeu de données\n",
    "doc  = open(\"data/LesMiserables_cleaned.txt\", \"r\")\n",
    "text = doc.read()\n",
    "text = text.split(\" \")\n",
    "text_serie = pd.Series(text)\n",
    "text_serie.head(10)\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding\n",
    "text_lb_miserables = LabelEncoder()\n",
    "X_hot_miserables = text_lb_miserables.fit_transform(text_serie.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100648,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nombre de mots du texte\n",
    "np.shape(X_hot_miserables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2955,  180, 5635, ...,  303,  608,    0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_hot_miserables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10215"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Taille du vocabulaire\n",
    "np.max(X_hot_miserables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['constant']\n",
      "[9667]\n"
     ]
    }
   ],
   "source": [
    "print(text_lb_miserables.inverse_transform([1995]))\n",
    "print(text_lb_miserables.transform([\"valjean\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Article Wikipédia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chargement du jeu de données\n",
    "doc  = open(\"data/Clapton_cleaned.txt\", \"r\")\n",
    "text = doc.read()\n",
    "text = text.split(\" \")\n",
    "text_serie = pd.Series(text)\n",
    "text_serie.head(10)\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding\n",
    "text_lb_clapton = LabelEncoder()\n",
    "X_hot_clapton = text_lb_clapton.fit_transform(text_serie.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5700,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nombre de mots du texte\n",
    "np.shape(X_hot_clapton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 583,  312, 1058, ..., 1481,  470,    0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_hot_clapton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1747"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Taille du vocabulaire\n",
    "np.max(X_hot_clapton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acoustique']\n",
      "[583]\n"
     ]
    }
   ],
   "source": [
    "print(text_lb_clapton.inverse_transform([21]))\n",
    "print(text_lb_clapton.transform([\"eric\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paramètres utilisés pour l'entraînement des modèles\n",
    "params = {\n",
    "    \"N\" : 2,\n",
    "    \"n_epochs\" : 20,\n",
    "    \"alpha\" : 10**(-3),\n",
    "    \"negative\" : 5,\n",
    "    \"d\" : 10,\n",
    "    'n_parallel' : 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevision(mot,Min,Mout,text_lb,p):\n",
    "    \"\"\" Permet d'effectuer la tâche de prédiction à partir de Min et Mout\n",
    "    mot (str) : mot à prédire\n",
    "    Min (array) : matrice des poids du réseau de neurones\n",
    "    Mout (array) : matrice des poids du réseau de neurones\n",
    "    text_lb : encodeur (pour permettre d'associer les mots à leur identifiant)\n",
    "    p : les p mots les plus probables\n",
    "    \n",
    "    return : p prédictions des mots les plus probables du contexte\n",
    "    \"\"\"\n",
    "    \n",
    "    index = text_lb.transform([mot])[0]\n",
    "    sub_Min = Min[index]\n",
    "    \n",
    "    prev = np.dot(sub_Min,Mout.T)\n",
    "    prev = np.exp(prev)/(np.sum(np.exp(prev)))\n",
    "    \n",
    "    index_prev = np.flip(np.argsort(prev))[:p]\n",
    "\n",
    "    word_prev = text_lb.inverse_transform(index_prev)\n",
    "    \n",
    "    return word_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme non parallélisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "Pour l'algorithme non parallélisé, on travaille uniquement avec le petit jeu de données, l'entraînement du second étant beaucoup trop long.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=time.time()\n",
    "Min,Mout = train_word2vec(X=X_hot_clapton, params=params)\n",
    "dt=time.time()-t0\n",
    "print(\"temps pour effectuer %d epochs : %s\" % (params['n_epochs'],dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prevision(\"eric\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"clapton\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"guitare\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"cocaine\",Min,Mout,text_lb_clapton,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "Un premier constat (que l'on observera également par la suite) est que l'entraînement d'un tel réseau de neurones est relativement complexe et nécessite une certaine quantité de travail en preprocessing des données. Ici, il semble que l'algorithme ait convergé en 20 epochs mais on observe que la surabondance de certains mots ('à', 'le', 'de', 'un' etc.) a tendance à fausser les résultats d'entraînement. Il convient donc de les supprimer en supposant qu'ils ne sont pas informatifs puisque prépondérants dans les contextes de la quasi totalité des mots des corpus de texte. On observe tout de même une certaine logique dans les prédictions ici : il s'agit d'un article sur Eric Clapton et le mot \"clapton\" se situe dans le contexte du mot \"eric\", de même les mots \"songs\" et \"meurt\" se situent dans le contexte du mot \"cocaine\" ce qui semble logique. (Remarque : ces résultats peuvent varier en ré éxécutant le notebook).\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "L'autre résultat est que plus d'une minute est nécessaire à l'entraînement du réseau (pour effectuer 20 epochs). Observons donc le gain dû à la parallélisation.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hogwild parallélisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = mod.get_function(\"parallel\")\n",
    "t0=time.time()\n",
    "Min, Mout = train_word2vec_parallel(X=X_hot_clapton, params=params, func=func)\n",
    "dt=time.time()-t0\n",
    "print(\"temps pour effectuer %d epochs : %s\" % (params['n_epochs'],dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "Un gain de temps d'un facteur <b>82</b> est donc obtenu via l'amélioration (ce résultat peut varier selon les éxecutions du notebook). C'est loin du facteur 1000 espéré mais l'amélioration est tout de même notable. La perte de performance est dûe notamment au transit d'informations parfois lourd entre les threads ce qui pourrait être optimisé. En effet, à chaque étape les matrices des poids de tout le réseau sont envoyées aux threads. Il serait raisonnable de ne travailler qu'avec des sous matrices en séléctionnant les lignes qui vont être utiles à l'entraînement. Le problème est que, du fait du negative sampling, on ne sait pas à l'avance quels mots vont être utilisés pour l'entraînement.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prevision(\"eric\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"clapton\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"guitare\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"cocaine\",Min,Mout,text_lb_clapton,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il semble que l'algorithme n'a pas convergé. Vérifions avec plus d'epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"N\" : 2,\n",
    "    \"n_epochs\" : 25,\n",
    "    \"alpha\" : 10**(-3),\n",
    "    \"negative\" : 5,\n",
    "    \"d\" : 10,\n",
    "    'n_parallel' : 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = mod.get_function(\"parallel\")\n",
    "t0=time.time()\n",
    "Min, Mout = train_word2vec_parallel(X=X_hot_clapton, params=params, func=func)\n",
    "dt=time.time()-t0\n",
    "print(\"temps pour effectuer %d epochs : %s\" % (params['n_epochs'],dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prevision(\"eric\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"clapton\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"guitare\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"cocaine\",Min,Mout,text_lb_clapton,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fois ci les mots prédits semblent plus cohérents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hogwild parallélisé et amélioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"N\" : 2,\n",
    "    \"n_epochs\" : 20,\n",
    "    \"alpha\" : 10**(-3),\n",
    "    \"negative\" : 5,\n",
    "    \"d\" : 10,\n",
    "    'n_parallel' : 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_2 = mod_2.get_function(\"parallel\")\n",
    "t0=time.time()\n",
    "Min, Mout = train_word2vec_parallel(X=X_hot_clapton, params=params, func=func_2)\n",
    "dt=time.time()-t0\n",
    "print(\"temps pour effectuer %d epochs : %s\" % (params['n_epochs'],dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "On observe malheureusement une perte de performance par rapport à la version seulement parallélisée de l'algorithme. Le gain de temps n'est plus que d'un rapport <b>25</b>. En fait c'est normal. Le code a été modifié pour que les opérations soient légérement plus nombreuses mais qu'elles soient majoritairement des produits entre matrices et non plus des produits entre vecteurs. L'idée est ensuite de jouer sur le fait que le produit matriciel peut-être optimisé en utilisant des librairies d'algèbre linéaire. Malheureusement, nous n'avons pas réussi à utiliser ces librairies via pycuda et nous avons programmé notre propre produit matriciel (très peu optimisé et très gourmand). D'après l'article l'utilisation de telles librairies permet de surpasser les implémentations de word2vec les plus rapides par ailleurs puisqu'avec leur matériel (bien plus puissants que nos ordinateurs), plusieurs centaines de millions de mots peuvent être appris par seconde (voir l'article).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prevision(\"eric\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"clapton\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"guitare\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"cocaine\",Min,Mout,text_lb_clapton,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du temps de calcul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est intéressant d'observer l'évolution des temps de calculs pour des paramètres V (taille du vocabulaire du corpus étudié) et d (taille de l'espace dans lequel on souhaite projeté nos données) différents qui vont avoir tendance à faire grandir l'espace mémoire utilisé. En particulier les matrices de poids du réseau de neurones sont de tailles V*d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"N\" : 2,\n",
    "    \"n_epochs\" : 1,\n",
    "    \"alpha\" : 10**(-3),\n",
    "    \"negative\" : 4,\n",
    "    \"d\" : 1,\n",
    "    'n_parallel' : 1000}\n",
    "\n",
    "TIMES = []\n",
    "D = []\n",
    "    \n",
    "for i in range(1,50):\n",
    "       \n",
    "    params[\"d\"]=i\n",
    "    \n",
    "    t0=time.time()\n",
    "    Min, Mout = train_word2vec_parallel(X=X_hot_miserables, params=params, func=func)\n",
    "    dt=time.time()-t0\n",
    "    \n",
    "    D.append(i)\n",
    "    TIMES.append(dt)\n",
    "    \n",
    "plot(D,TIMES)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"N\" : 2,\n",
    "    \"n_epochs\" : 1,\n",
    "    \"alpha\" : 10**(-3),\n",
    "    \"negative\" : 4,\n",
    "    \"d\" : 10,\n",
    "    'n_parallel' : 1000}\n",
    "\n",
    "V = []\n",
    "TIMES = []\n",
    "\n",
    "for i in range (1,50):\n",
    "    \n",
    "    doc  = open(\"data/LesMiserables_cleaned.txt\", \"r\")\n",
    "    text = doc.read()[:i*len(text)/50]\n",
    "    text = text.split(\" \")\n",
    "    text_serie = pd.Series(text)\n",
    "    doc.close()\n",
    "\n",
    "    text_lb_miserables = LabelEncoder()\n",
    "    X_hot_miserables = text_lb_miserables.fit_transform(text_serie.values)\n",
    "    \n",
    "    V.append(np.max(X_hot_miserables))\n",
    "    \n",
    "    t0=time.time()\n",
    "    Min, Mout = train_word2vec_parallel(X=X_hot_miserables, params=params, func=func)\n",
    "    dt=time.time()-t0\n",
    "    \n",
    "    TIMES.append(dt)\n",
    "    print(dt)\n",
    "    \n",
    "V = np.array(V)\n",
    "TIMES = np.array(TIMES)\n",
    "\n",
    "plot(V,TIMES)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "Ce projet a été pour nous, une première approche de la programmation GPU et de l'utilisation de CUDA. Il est très intéressant de pouvoir comprendre les mécanismes de parallélisation des calculs en programmant au moins une fois par nous même avec CUDA.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "Ce projet a été source de nombreuses difficultées pour nous. En premier lieu nous n'avions jamais codé en C et il a fallu rapidemment en comprendre les fondamentaux. Ensuite dans une première approche, l'utilisation de pycuda est relativement difficile (le débogage des fonctions sous CUDA en particulier a été très pénible). L'installation de pycuda nous a également fait perdre beaucoup de temps. De plus les améliorations de l'algorithme proposées dans l'article ne sont pas codées dans l'article, seul l'idée de la parallélisation est données et il a fallu construire l'algorithme par nous même.Nous sommes tout de même heureux d'avoir réussi à implémenter les techniques de l'article et d'avoir pu observer les gains de performances effectifs qu'ils entraînent. \n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "Il aurait été intéressant de pouvoir aller plus loin. Et les pistes suivantes pourraient permettre d'améliorer notre programme et de se rapprocher du code effectivement utilisé par Ji, Shihao, et al. Par exemple il convient d'adapter la parallélisation des calculs à la structure propre de son GPU et nous sommes restés dans un cadre général pour notre implémentation. Aussi il faudrait utiliser les librairies d'algébre linéaire (BLAS) pour constater des performances largement supérieures à celles obtenues. Les fonctions codées en C pourraient également facilement être améliorées mais le fait est que nous débutions dans la pratique du langage. Enfin pour que les performances de prédiction du réseau de neurones soient améliorées, il faudrait d'avantage travailler en amont sur les données notamment la suppression des mots prépondérants est une astuce cruciale.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "Le résultat principal de notre travail est donc le gain de performance d'un facteur de 80 obtenu en parallélisant l'algorithme word2vec. Cette méthode est à la pointe et permet à des clusters puissants d'entraîner jusque plusieurs centaines de millions de mots par secondes sans que l'on observe de perte de convergence et c'est une des clé du travail d'entraînement de bases de données massives.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
