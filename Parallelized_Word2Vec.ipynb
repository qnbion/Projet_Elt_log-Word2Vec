{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Element Logiciel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Après\n",
       "1     avoir\n",
       "2    débuté\n",
       "3      sous\n",
       "4        le\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On travaille sur un petit texte pour commencer extrait d'un article wikipedia\n",
    "text  = open(\"data/TheBeatles.txt\", \"r\") \n",
    "text = text.readlines()[0]\n",
    "\n",
    "#Preprocessing : on supprime la ponctuation\n",
    "not_alphabet=\"'?./§,;:!»«()…-\" \n",
    "for i in not_alphabet:\n",
    "    text = text.replace(i, \"\")\n",
    "text = text.split(\" \")\n",
    "text_serie = pd.Series(text)\n",
    "text_serie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "text_lb = LabelBinarizer()\n",
    "X_hot = text_lb.fit_transform(text_serie.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1300, 645)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_hot)\n",
    "#Le ième mot du texte est le mot j => X_hot_ij = 1\n",
    "#X_hot : n*V\n",
    "#n : nombre de mot dans le texte\n",
    "#V : nombre de mots différents dans le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.apply_along_axis(np.argmax, 1, X_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 40, 171, 263, ..., 596,   0,   0], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n",
    "#Il est plus simple de représenter le texte sous cette forme\n",
    "#X_i est l'identifiant du ième mot du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hogwild implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On commence par rédiger le code sans parallèlisation \n",
    "def sig(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def hog_loop(Min,Mout,alpha,wout,Nwin,negative):\n",
    "    #Min matrice dans V*d\n",
    "    #Mout matrice dans V*d\n",
    "    #wout un élément de 0:V-1\n",
    "    #Nwin n entiers de 0:V-1\n",
    "    V,d=Min.shape\n",
    "    N=Nwin.shape[0]\n",
    "    for i in range(N):\n",
    "        input_word = Nwin[i]\n",
    "        temp = np.array([0]*d)\n",
    "        for k in range(negative+1):\n",
    "            if k == 0:\n",
    "                target_word = wout\n",
    "                label = 1\n",
    "            else : \n",
    "                target_word = rd.randint(0,V-1)\n",
    "                label = 0\n",
    "            inn = np.dot(Min[input_word,:],Mout[target_word,:])\n",
    "            err = label - sig(inn)\n",
    "            temp =temp + err*Mout[target_word,:]\n",
    "            Mout[target_word,:] = Mout[target_word,:] + alpha*err*Min[input_word,:]\n",
    "        Min[input_word,:] = Min[input_word,:] + alpha*temp\n",
    "    return(Min,Mout)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(text,params):\n",
    "    \n",
    "    N = params['N']\n",
    "    n_epochs = params['n_epochs']\n",
    "    alpha = params['alpha']\n",
    "    negative = params['negative']\n",
    "    d = params['d']\n",
    "    \n",
    "    V = np.max(X)+1\n",
    "    n_words = np.shape(X)[0]\n",
    "    \n",
    "    Min = np.array([rd.random() for i in range(V*d)]).reshape((V,d))\n",
    "    Mout = np.array([rd.random() for i in range(V*d)]).reshape((V,d))\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        epochs_order = np.array([t for t in range(N,n_words-N)])\n",
    "        np.random.shuffle(epochs_order)\n",
    "        \n",
    "        for j in epochs_order:\n",
    "            \n",
    "            wout = X[j]\n",
    "            \n",
    "            Nwin = []\n",
    "            for k in range(N):\n",
    "                Nwin.append(X[j-N+k])\n",
    "            for k in range(N):\n",
    "                Nwin.append(X[j+k+1])\n",
    "            Nwin = np.array(Nwin)\n",
    "                \n",
    "            Min, Mout = hog_loop(Min,Mout,alpha,wout,Nwin,negative)\n",
    "            \n",
    "    \n",
    "    return Min,Mout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"N\" : 2,\n",
    "    \"n_epochs\" : 100,\n",
    "    \"alpha\" : 10**(-3),\n",
    "    \"negative\" : 4,\n",
    "    \"d\" : 10\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps pour effectuer 100 epochs : 55.22755193710327\n",
      "[[-1.00512081 -0.04997956 -0.95965587 ... -0.99662608 -1.19826614\n",
      "  -1.31668949]\n",
      " [-0.4440905   0.3948632  -0.11329952 ... -0.45706359 -0.13268179\n",
      "  -0.38565743]\n",
      " [-0.20827107  0.11476191 -0.47287415 ... -0.31884309  0.17539378\n",
      "   0.28877732]\n",
      " ...\n",
      " [ 0.47178256 -0.20667278  0.55916136 ... -0.18989756  0.32222691\n",
      "   0.1966601 ]\n",
      " [-0.0050664   0.40778788 -0.18933195 ... -0.17228613  0.0366665\n",
      "   0.47896505]\n",
      " [-0.74047766 -0.22102198  0.17123655 ... -0.03146855 -0.50424612\n",
      "  -0.66055687]]\n"
     ]
    }
   ],
   "source": [
    "t=time.time()\n",
    "Min,Mout = train_word2vec(text=X, params=params)\n",
    "t=time.time()-t\n",
    "print(\"temps pour effectuer %d epochs : %s\" % (params['n_epochs'],t))\n",
    "print(Min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in Pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMQ Dupre : \n",
    "\n",
    "regarder \n",
    "// ++i i++\n",
    "   for(auto it: temp)\n",
    "       *it = 0;\n",
    "    \n",
    "for(float* it = temp; it != ; ++it)\n",
    "      *it = 0;\n",
    "      \n",
    "RMQ : peut-être pas la peine de passer random en entier à chaque loop\n",
    "\n",
    "Faut-il plus de rnadom ?? -> r = (1664525*r+1013904223) % 4294967296\n",
    "\n",
    "malloc --> free ? OUI : simplement free(variable);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "\n",
    "#include <time.h>\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <curand.h>\n",
    "#include <math.h>\n",
    "\n",
    "__device__ void loop(float *Min, float *Mout, float alpha, int *Nwin, int wout, int V, int d, int N, int negative, int random){\n",
    "    \n",
    "    /*\n",
    "    HOGWILD LOOP\n",
    "    \n",
    "    float *Min : initialisé en dehors de pycuda, poids de la PREMIERE couche du RN (attention float * et non float **) (V*d)\n",
    "    float *Mout : initialisé en dehors de pycuda, poids de la DEUXIEME couche du RN (attention float * et non float **) (V*d)\n",
    "    float alpha : learning rate\n",
    "    int *Nwin : 2*N input du contexte de l'output\n",
    "    int wout : output\n",
    "    int V : taille du vocabulaire (nombre de mots différents)\n",
    "    int d : taille de l'espace de représentation\n",
    "    int N : taille du contexte utilisé pour l'apprentissage (de chaque coté donc 2*N en tout)\n",
    "    int negative : nb de negative utilisé pour l'apprentissage\n",
    "    int random : un entier aléatoire (difficulté pour générer de l'aléat sur le device) pour les negative\n",
    "    \n",
    "    Une boucle de l'Algo 1 de l'article. Permet de mettre à jour Min et Mout pour un wout (et son contexte associé).\n",
    "    */\n",
    "\n",
    "    /* Init variables */\n",
    "    float* temp;\n",
    "    temp = (float *)malloc(sizeof(float)*d);\n",
    "    int label;\n",
    "    int target_word;\n",
    "    float inn;\n",
    "    float err;\n",
    "    int r;\n",
    "    \n",
    "    /* Boucle principale sur les 2*N inputs*/\n",
    "    for(int i=0;i<2*N;i++){\n",
    "        int input_word = Nwin[i];\n",
    "        for(int j=0;j<d;j++){\n",
    "            temp[j]=0;\n",
    "        }\n",
    "        for(int k=0; k<negative+1;k++){\n",
    "            if (k==0){\n",
    "                target_word = wout;\n",
    "                label = 1;\n",
    "            } else {\n",
    "                /* negative sampling */\n",
    "                r = (1664525*r+1013904223) % 4294967296;\n",
    "                target_word = r%V;\n",
    "                label = 0;\n",
    "            }\n",
    "            inn = 0;\n",
    "            for(int j=0;j<d;j++){\n",
    "                inn = inn + Min[input_word*d+j]*Mout[target_word*d+j];\n",
    "            }\n",
    "            err = label-(1/(1+exp(-inn)));\n",
    "            for(int j=0;j<d;j++){\n",
    "                temp[j] = temp[j]+err*Mout[target_word*d+j];\n",
    "            }\n",
    "            for(int j=0;j<d;j++){\n",
    "                Mout[target_word*d+j] = Mout[target_word*d+j]+alpha*err*Min[input_word*d+j];\n",
    "            }  \n",
    "        }\n",
    "        for(int j=0;j<d;j++){\n",
    "            Min[input_word*d+j] = Min[input_word*d+j]+alpha*temp[j]; \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    free(temp);\n",
    "}\n",
    "\n",
    "\n",
    "__global__ void parallel(float *Min, float* Mout, int random, int* cst_int, float* cst_float, int* targets, int* contexts) {\n",
    "\n",
    "    /*\n",
    "    Parallélisation\n",
    "    float *Min : Min initialisé\n",
    "    float *Mout : Mout initialisé\n",
    "    int* random : un entier aléatoire généré hors du device\n",
    "    int* cst_int : les constantes entières utiles (V,d,negative,N)\n",
    "    float* cst_float : les constantes float utiles (alpha)\n",
    "    int* targets : on va travailler sur wout=targets[idx]\n",
    "    int* context : le contexte associé sera contexts[idx]\n",
    "    */\n",
    "\n",
    "    /*préparation des paramètres pour la fonction loop*/\n",
    "    \n",
    "    \n",
    "    /*constantes entières passés depuis le code python*/\n",
    "    int V = cst_int[0];\n",
    "    int d = cst_int[1];\n",
    "    int negative = cst_int[2];\n",
    "    int N = cst_int[3];\n",
    "\n",
    "    /*constante float passé depuis python*/\n",
    "    float alpha = cst_float[0];\n",
    "    \n",
    "    /*l'index correspond au thread et indique simplement le wout sur lequel on travaille*/\n",
    "    int index = threadIdx.x; \n",
    "    int wout = targets[index];\n",
    "    \n",
    "    /*Context de wout*/\n",
    "    int *Nwin;\n",
    "    Nwin = (int *)malloc(sizeof(int)*N*2);\n",
    "    for (int i=0;i<2*N;i++){\n",
    "        Nwin[i] = contexts[index*2*N+i];\n",
    "    }\n",
    "    \n",
    "    random = random+index;\n",
    "    \n",
    "    loop(Min,Mout,alpha,Nwin,wout,V,d,N,negative,random);\n",
    "    \n",
    "    free(Nwin);\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec_parallel(text, params, func):\n",
    "    N = params['N']\n",
    "    n_epochs = params['n_epochs']\n",
    "    alpha = params['alpha']\n",
    "    negative = params['negative']\n",
    "    d = params['d']\n",
    "    \n",
    "    V = np.max(X)+1\n",
    "    n_words = np.shape(X)[0]\n",
    "    \n",
    "    cst_int = np.array([V,d,negative,N])\n",
    "    cst_float = np.array([alpha])\n",
    "    \n",
    "    cst_int = cst_int.astype(np.int32)\n",
    "    cst_float = cst_float.astype(np.float32)\n",
    "    \n",
    "    Min = np.array([rd.random() for i in range(V*d)]).reshape((V,d))\n",
    "    Mout = np.array([rd.random() for i in range(V*d)]).reshape((V,d))\n",
    "    \n",
    "    Min = Min.astype(np.float32)\n",
    "    Mout = Mout.astype(np.float32)\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        epochs_order = np.array([t for t in range(N,n_words-N)])\n",
    "        np.random.shuffle(epochs_order)\n",
    "        \n",
    "        for j in range(n_words//1000+1):\n",
    "            sub_epochs_order = epochs_order[j*1000:(j+1)*1000]\n",
    "            \n",
    "            n_threads = np.shape(sub_epochs_order)[0]\n",
    "            \n",
    "            contexts=[]\n",
    "            targets=[]\n",
    "            \n",
    "            for k in sub_epochs_order : \n",
    "                targets.append(X[k])\n",
    "                Nwin=[]\n",
    "                for l in range(N):\n",
    "                    Nwin.append(X[k-N+l])\n",
    "                for l in range(N):\n",
    "                    Nwin.append(X[k+l+1])\n",
    "                contexts.append(Nwin)\n",
    "            \n",
    "            targets = np.array(targets)\n",
    "            contexts = np.array(contexts)\n",
    "            \n",
    "            targets = targets.astype(np.int32)\n",
    "            contexts = contexts.astype(np.int32)\n",
    "            \n",
    "            r = np.array([rd.randint(1,1000000)])\n",
    "            r = r.astype(np.int32)\n",
    "            \n",
    "            Min_gpu = cuda.mem_alloc(Min.nbytes)\n",
    "            Mout_gpu = cuda.mem_alloc(Mout.nbytes)\n",
    "            r_gpu = cuda.mem_alloc(r.nbytes)\n",
    "            cst_int_gpu = cuda.mem_alloc(cst_int.nbytes)\n",
    "            cst_float_gpu = cuda.mem_alloc(cst_float.nbytes)\n",
    "            targets_gpu = cuda.mem_alloc(targets.nbytes)\n",
    "            contexts_gpu = cuda.mem_alloc(contexts.nbytes)\n",
    "\n",
    "            cuda.memcpy_htod(Min_gpu, Min)\n",
    "            cuda.memcpy_htod(Mout_gpu, Mout)\n",
    "            cuda.memcpy_htod(r_gpu, r)\n",
    "            cuda.memcpy_htod(cst_int_gpu,cst_int)\n",
    "            cuda.memcpy_htod(cst_float_gpu,cst_float)\n",
    "            cuda.memcpy_htod(contexts_gpu,contexts)\n",
    "            cuda.memcpy_htod(targets_gpu,targets)\n",
    "            \n",
    "            func(Min_gpu, Mout_gpu, r_gpu, cst_int_gpu, cst_float_gpu, targets_gpu, contexts_gpu, block=(n_threads,1,1))\n",
    "            \n",
    "            cuda.memcpy_dtoh(Min, Min_gpu)\n",
    "            cuda.memcpy_dtoh(Mout, Mout_gpu)\n",
    "\n",
    "\n",
    "    \n",
    "    return Min,Mout\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = mod.get_function(\"parallel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps d'execution de 100 epochs\n",
      "1.3676567077636719\n",
      "[[ 1.0032915e+00  9.6302933e-01  1.1017778e+00 ...  1.0325381e+00\n",
      "   4.6229160e-01 -4.9877691e-01]\n",
      " [ 1.3147280e-01  9.8225397e-01  1.9095585e-01 ...  5.9697044e-01\n",
      "   8.7365127e-01 -4.6755811e-03]\n",
      " [ 4.8149750e-01  7.8118622e-01  4.8062629e-01 ...  3.6831555e-01\n",
      "   6.5155959e-01  1.5294552e-01]\n",
      " ...\n",
      " [ 3.0932373e-01  4.8600227e-01  7.4486464e-01 ...  9.8725528e-02\n",
      "   6.3812017e-01  5.5493981e-01]\n",
      " [ 2.6830742e-01  1.1516989e-01  6.2688410e-01 ...  5.8861768e-01\n",
      "   6.8537962e-01  3.7184832e-01]\n",
      " [ 9.0043718e-01  6.2832558e-01  1.6175668e-01 ...  3.6975247e-01\n",
      "   8.0342275e-01 -1.4484519e-04]]\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "Min, Mout = train_word2vec_parallel(text=X, params=params, func=func)\n",
    "dt=time.time()-t0\n",
    "print(\"temps d'execution de %d epochs\" % params[\"n_epochs\"])\n",
    "print(dt)\n",
    "print(Min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"10    : 0.00421\"\"\"\n",
    "\"\"\"100   : 0.00507\"\"\"\n",
    "\"\"\"1000  : 0.00400\"\"\"\n",
    "\"\"\"see https://stackoverflow.com/questions/9985912/how-do-i-choose-grid-and-block-dimensions-for-cuda-kernels\"\"\"\n",
    "\"\"\"https://github.com/IntelLabs/pWord2Vec\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper amelioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quent\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:200: UserWarning: The CUDA compiler succeeded, but said the following:\n",
      "kernel.cu\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod_2 = SourceModule(\"\"\"\n",
    "\n",
    "\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <curand.h>\n",
    "#include <math.h>\n",
    "\n",
    "\n",
    "__device__ void multiplication(float* A, float* B, float* C, int* dim){\n",
    "    \n",
    "    int lA = dim[0];\n",
    "    int cA = dim[1];\n",
    "    int lB = dim[2];\n",
    "    int cB = dim[3];\n",
    "    \n",
    "    if (cA!=lB){\n",
    "        return ;\n",
    "    }\n",
    "    \n",
    "    for(int i=0; i<lA ; i++){\n",
    "        for(int j=0; j<cB ; j++){\n",
    "            C[i*cB+j] = 0;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for(int i=0; i<lA ; i++){\n",
    "        for(int j=0; j<cB ; j++){\n",
    "            for(int k=0 ; k<cA ; k++){\n",
    "                C[i*cB+j] = C[i*cB+j] + A[i*cA+k]*B[k*cB+j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "__device__ void transpose(float* A, float* TA, int* dim){\n",
    "    \n",
    "    int lA = dim[0];\n",
    "    int cA = dim[1];\n",
    "      \n",
    "    for(int i=0; i<cA ; i++){\n",
    "        for(int j=0; j<lA ; j++){\n",
    "            TA[i*lA+j] = A[j*cA+i];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "__device__ void loop(float *Min, float *Mout, float alpha, int *Nwin, int wout, int V, int d, int N, int negative, int r){\n",
    "\n",
    "\n",
    "    /*Calcul des target_words (wout et negative sample)*/\n",
    "    int *target_words;\n",
    "    target_words = (int *)malloc(sizeof(int)*(negative+1));\n",
    "    for(int i=0; i<negative+1;i++){\n",
    "        if (i==0){\n",
    "            target_words[i] = wout;\n",
    "        } else {\n",
    "            r = (1664525*r+1013904223) % 4294967296;\n",
    "            target_words[i] = r%V;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /*label*/\n",
    "    float *label;\n",
    "    label = (float *)malloc(sizeof(int)*(2*N*(negative+1)));\n",
    "    for (int i=0; i<2*N ; i++){\n",
    "        for (int j=0 ; j<negative+1;j++){\n",
    "            if (j==0){\n",
    "                label[i*(negative+1)+j] = 1;\n",
    "            } else {\n",
    "                label[i*(negative+1)+j] = 0;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /*sub_Min*/\n",
    "    float* sub_Min;\n",
    "    sub_Min = (float *)malloc(sizeof(int)*2*N*d);\n",
    "    for (int i=0; i<2*N ; i++){\n",
    "        for (int j=0 ; j<d ; j++){\n",
    "            sub_Min[i*d+j] = Min[Nwin[i]*d+j];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /*sub_Mout*/\n",
    "    float* sub_Mout;\n",
    "    sub_Mout = (float *)malloc(sizeof(int)*(negative+1)*d);\n",
    "    for (int i=0; i<negative+1 ; i++){\n",
    "        for (int j=0 ; j<d ; j++){\n",
    "            sub_Mout[i*d+j] = Mout[target_words[i]*d+j];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /*sub_Mout_transpose*/\n",
    "    float* sub_Mout_transpose;\n",
    "    sub_Mout_transpose = (float *)malloc(sizeof(int)*d*(negative+1));\n",
    "    int dim_sub_Mout[2];\n",
    "    dim_sub_Mout[0] = negative+1;\n",
    "    dim_sub_Mout[1] = d;\n",
    "    transpose(sub_Mout, sub_Mout_transpose, dim_sub_Mout);\n",
    "    \n",
    "    /*INN*/\n",
    "    float* INN;\n",
    "    INN = (float *)malloc(sizeof(int)*2*N*(negative+1));\n",
    "    int dim[4];\n",
    "    dim[0] = 2*N;\n",
    "    dim[1] = d;\n",
    "    dim[2] = d;\n",
    "    dim[3] = negative+1;\n",
    "    multiplication(sub_Min, sub_Mout_transpose, INN, dim);\n",
    "    \n",
    "    /*ERR*/\n",
    "    float* ERR;\n",
    "    ERR = (float *)malloc(sizeof(int)*2*N*(negative+1));\n",
    "    for (int i=0; i<2*N ; i++){\n",
    "        for (int j=0 ; j<negative+1;j++){\n",
    "            ERR[i*(negative+1)+j] = label[i*(negative+1)+j] - INN[i*(negative+1)+j];\n",
    "        }\n",
    "    }  \n",
    "    \n",
    "    /*TEMP*/\n",
    "    float* TEMP;\n",
    "    TEMP = (float *)malloc(sizeof(int)*2*N*d);\n",
    "    dim[0] = 2*N;\n",
    "    dim[1] = negative+1;\n",
    "    dim[2] = negative+1;\n",
    "    dim[3] = d;\n",
    "    multiplication(ERR, sub_Mout, TEMP, dim);\n",
    "    \n",
    "    /*MAJ Mout*/\n",
    "    float* ERR_transpose_alpha;\n",
    "    ERR_transpose_alpha = (float *)malloc(sizeof(int)*(negative+1)*2*N);\n",
    "    for (int i=0; i<negative+1 ; i++){\n",
    "        for (int j=0 ; j<2*N;j++){\n",
    "            ERR_transpose_alpha[i*2*N+j] = alpha*ERR[j*(negative+1)+i]; \n",
    "        }\n",
    "    }\n",
    "    dim[0] = negative+1;\n",
    "    dim[1] = 2*N;\n",
    "    dim[2] = 2*N;\n",
    "    dim[3] = d;\n",
    "    multiplication(ERR_transpose_alpha, sub_Min, sub_Mout, dim);\n",
    "    for (int i=0; i<negative+1 ; i++){\n",
    "        for (int j=0 ; j<d;j++){\n",
    "            Mout[target_words[i]*d+j] = sub_Mout[i*d+j]; \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /*MAJ Min*/\n",
    "    for(int i = 0;i<2*N;i++){\n",
    "        for(int j = 0;j<d;j++){\n",
    "            Min[Nwin[i]*d+j] = alpha*TEMP[i*d+j];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    free(target_words);\n",
    "    free(label);\n",
    "    free(sub_Min);\n",
    "    free(sub_Mout);\n",
    "    free(sub_Mout_transpose);\n",
    "    free(INN);\n",
    "    free(ERR);\n",
    "    free(TEMP);\n",
    "    free(ERR_transpose_alpha);\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "__global__ void parallel(float *Min, float* Mout, int random, int* cst_int, float* cst_float, int* targets, int* contexts) {\n",
    "\n",
    "    /*constantes entières passés depuis le code python*/\n",
    "    int V = cst_int[0];\n",
    "    int d = cst_int[1];\n",
    "    int negative = cst_int[2];\n",
    "    int N = cst_int[3];\n",
    "\n",
    "    /*constante float passé depuis python*/\n",
    "    float alpha = cst_float[0];\n",
    "    \n",
    "    /*l'index correspond au thread et indique simplement le wout sur lequel on travaille*/\n",
    "    int index = threadIdx.x; \n",
    "    int wout = targets[index];\n",
    "    \n",
    "    /*Context de wout*/\n",
    "    int *Nwin;\n",
    "    Nwin = (int *)malloc(sizeof(int)*N*2);\n",
    "    for (int i=0;i<2*N;i++){\n",
    "        Nwin[i] = contexts[index*2*N+i];\n",
    "    }\n",
    "    \n",
    "    random = random+index;\n",
    "    \n",
    "    loop(Min,Mout,alpha,Nwin,wout,V,d,N,negative,random);\n",
    "    \n",
    "}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test multiplication\n",
    "A = np.array([[1,2],[3,4],[5,6]])\n",
    "B = np.array([[1,2,3],[3,2,1]])\n",
    "C = np.array([[1,0,0],[1,0,0],[1,0,0]])\n",
    "dim = np.array([np.shape(A)[0],\n",
    "      np.shape(A)[1],\n",
    "      np.shape(B)[0],\n",
    "      np.shape(B)[1]])\n",
    "\n",
    "A = A.astype(np.float32)\n",
    "B = B.astype(np.float32)\n",
    "C = C.astype(np.float32)\n",
    "dim = dim.astype(np.int32)\n",
    "\n",
    "A_gpu = cuda.mem_alloc(A.nbytes)\n",
    "B_gpu = cuda.mem_alloc(B.nbytes)\n",
    "C_gpu = cuda.mem_alloc(C.nbytes)\n",
    "dim_gpu = cuda.mem_alloc(dim.nbytes)\n",
    "\n",
    "cuda.memcpy_htod(A_gpu, A)\n",
    "cuda.memcpy_htod(B_gpu, B)\n",
    "cuda.memcpy_htod(C_gpu, C)\n",
    "cuda.memcpy_htod(dim_gpu, dim)\n",
    "\n",
    "func = mod.get_function(\"multiplication\")\n",
    "\n",
    "func(A_gpu, B_gpu, C_gpu, dim_gpu, block=(1,1,1))\n",
    "\n",
    "cuda.memcpy_dtoh(C,C_gpu)\n",
    "\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
