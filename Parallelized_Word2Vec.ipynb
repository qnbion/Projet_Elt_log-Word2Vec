{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usefull packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import copy\n",
    "import time\n",
    "\n",
    "#pycuda\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Element Logiciel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autour de l'article, présentation du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "L'idée du projet est de paralléliser l'algorithme word2vec. On s'appuie pour cela sur l'article de Ji, Shihao, et al. <i>Parallelizing word2vec in shared and distributed memory</i>. Nous avons utilisé <i> pycuda </i> pour effectuer la programmation GPU. Dans ce notebook on utilise donc le langage Python comme langage d'environnement : on s'en sert principalement pour préparer les données, présenter les résultats et exécuter les fonctions à paralléliser. L'algorithme word2vec est en revanche codé en C.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "L'algorithme word2vec permet de représenter les mots d'un texte dans un espace de petite dimension (typiquement R<sup>d</sup> avec d de l'ordre de 10). Il permet donc de passer d'une représentation des données sous forme one-hot (où la taille de l'espace des données est égale à la taille du vocabulaire du corpus de texte étudié) à une représentaion dans R<sup>d</sup>. L'idée est que, des mots proches sémantiquement (on les suppose proches si ils sont proches dans le texte) doivent être proches dans l'espace de projection R<sup>d</sup> (au sens euclidien cette fois).\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "L'architecture de word2vec est un réseau de neurones à une couche cachée. Si l'on souhaite que notre espace de projection soit R<sup>d</sup>, on choisira d neurones dans la couche cachée. Les couches d'entrée et de sortie sont de taille V où V est la taille du vocabulaire du corpus de texte étudié. Pour construire notre matrice de projection dans R<sup>d</sup>, on entraîne ce réseau de neurones via une tâche de prédiction annexe. Pour un mot en entrée, le réseau doit prédire le où les mots les plus proches sémantiquement. La matrice des poids de passage de la couche d'input à la couche cachée sera alors notre matrice de projection. On construit donc un ensemble d'entraînement (X,Y) où on fixe la taille N du contexte utilisé et les couples (Xi,Yi) sont determinés de la manière suivante. Pour un mot Xi du texte et pour les mots Yi1, Yi2,...,YiN qui sont autour du mot Xi dans le texte les couples (Xi,Yi1),...(Xi,YiN) forment la base d'entraînement.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "L'entraînement de ce réseau nécessite l'implémentation de l'algorithme de descente de gradient. L'article met en avant les inconvénients de cet algorithme dans ce cas précis, en particulier l'évaluation de la fonction de perte et de ses gradients est très gourmande (le nombre de calculs étant proportionnel au nombre de mots dans le texte). On préférera donc utiliser la méthode de descente de gradient stochastique (avec negative sampling) qui n'utilise qu'une faible proportion des mots du texte à chaque évaluation des oracles. A chaque étape la méthode de gradient stochastique met à jour les poids correspondant à un mot Xi et à un mot de son contexte Yij. Le negative sampling consiste à prendre des mots au hasard dans le texte à chaque étape et à mettre à jour les poids correspondant en considérant que ces mots ne sont pas dans le contexte de Xi.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "Dans l'article, les auteurs proposent, pour accélerer l'implémentation de la descente de gradient stochastique, de paralléliser les boucles de mise à jour des poids du réseau (voir section III de l'article). Cela risque de créer des conflits (puisque les mêmes poids risquent d'être mis à jour au même moment) mais on s'en affranchit, cela peut réduire théoriquement les taux de convergence de l'algorithme mais permet de gagner en vitesse (d'un facteur du nombre de parallélisation). On observe que pour des grands textes et à condition de ne pas utiliser un nombre de threads trop élevé, les conflits sont rares et n'empechent pas la convergence. L'algorihme que l'on parallélise est l'implémentation d'Hogwild (voir <i>Algorithm 1</i> de l'article).\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "La deuxième amélioration proposée dans l'article est double. Elle consiste à exploiter plus encore les propriétés locales des informations utilisées à chaque boucle de mise à jour. L'idée est de partager le <i>negative sampling</i> entre les mots d'un même contexte. Cela va permettre de passer des opérations de produits scalaires (opérations BLAS-1)  utilisés massivement dans l'implémentation d'Hogwild à des produits matriciels (opérations BLAS-3) en théorie plus efficaces en utilisant des bibliothèques performatantes d'algèbre linéaire.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "On s'attache dans ce projet à mettre en place ces deux techniques sur un petit jeu de données et d'observer les gains effectivement obtenus.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hogwild algorithm in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "On commence par coder l'algorithme sous Python. Cette première implémentation de la méthode ne sera donc pas parallélisée et nous permettra de comparer les résultats et la vitesse de calcul avec le code parallélisé sous pycuda. On rédige volontairement un code non optimisé (nous n'utilisons pas les produits matriciels de numpy par exemple) ce qui nous permettra de constater effectivement les gains de performance dûs à l'utilisation de CUDA.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    \"\"\" fonction sigmoid.\n",
    "    x : un réel (float)\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def hog_loop(Min,Mout,alpha,wout,Nwin,negative):\n",
    "    \"\"\" Une boucle permettant la mise à jour des poids du réseau à partir d'un mot et de son contexte\n",
    "    Min (array) : La matrice des poids du réseau de neurones reliant la couche d'input à la couche cachée (taille V*d)\n",
    "    Mout (array) : La matrice des poids du réseau de neurones reliant la couche cachée à la couche d'output (taille V*d)\n",
    "    alpha (float) : learning rate\n",
    "    wout (int) : un entier représentant le mot qui va être mis à jour\n",
    "    Nwin (array) : les mots du contexte de wout\n",
    "    negative (int) : taille du negative sampling\n",
    "    \n",
    "    return : Min, Mout les poids mis à jour\n",
    "    \"\"\"\n",
    "    V,d = Min.shape\n",
    "    N = Nwin.shape[0]\n",
    "    for i in range(N):\n",
    "        input_word = Nwin[i]\n",
    "        temp = np.array([0.0]*d)\n",
    "        for k in range(negative+1):\n",
    "            if k == 0:\n",
    "                target_word = wout\n",
    "                label = 1\n",
    "            else :\n",
    "                #negative sampling\n",
    "                target_word = rd.randint(0,V-1)\n",
    "                label = 0\n",
    "            inn = 0\n",
    "            for j in range(d):\n",
    "                inn = inn + Min[input_word,j]*Mout[target_word,j]\n",
    "            err = label - sig(inn)\n",
    "            for j in range(d):\n",
    "                temp[j] = temp[j] + err*Mout[target_word,j]\n",
    "            for j in range(d):\n",
    "                #MAJ Mout\n",
    "                Mout[target_word,j] = Mout[target_word,j] + alpha*err*Min[input_word,j]\n",
    "        for j in range(d):\n",
    "            #MAJ Min\n",
    "            Min[input_word,j] = Min[input_word,j] + alpha*temp[j]\n",
    "    return(Min,Mout)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(X,params):\n",
    "    \"\"\" fonction d'entraînement du reseau de neurones\n",
    "    X (array) : texte constituant la base d'entraînement\n",
    "    params (dict) :  N, n_epochs, alpha, negative, d\n",
    "    N (int) : taille du contexte d'un mot \n",
    "    n_epochs (int) : nombre d'entraînement complet du réseau\n",
    "    alpha (float) : learning rate\n",
    "    negative (int) : taille du negative sampling\n",
    "    d (int) : taille de l'espace de représentation\n",
    "    \n",
    "    return : Min, Mout (array) les matrices de poids du réseau entraîné\n",
    "    \"\"\"\n",
    "    \n",
    "    N = params['N']\n",
    "    n_epochs = params['n_epochs']\n",
    "    alpha = params['alpha']\n",
    "    negative = params['negative']\n",
    "    d = params['d']\n",
    "    \n",
    "    V = np.max(X)+1 #taille du vocabulaire\n",
    "    n_words = np.shape(X)[0] #nombre de mots du corpus\n",
    "    \n",
    "    #Initialisation du réseau\n",
    "    Min = np.array([rd.random() for i in range(V*d)]).reshape((V,d))\n",
    "    Mout = np.array([rd.random() for i in range(V*d)]).reshape((V,d))\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        epochs_order = np.array([t for t in range(N,n_words-N)])\n",
    "        np.random.shuffle(epochs_order)\n",
    "        \n",
    "        for j in epochs_order:\n",
    "            \n",
    "            wout = X[j]\n",
    "            \n",
    "            #Calcul du contexte de wout\n",
    "            Nwin = []\n",
    "            for k in range(N):\n",
    "                Nwin.append(X[j-N+k])\n",
    "            for k in range(N):\n",
    "                Nwin.append(X[j+k+1])\n",
    "            Nwin = np.array(Nwin)\n",
    "            \n",
    "            #Maj de wout et de son contexte\n",
    "            Min, Mout = hog_loop(Min,Mout,alpha,wout,Nwin,negative)\n",
    "            \n",
    "    \n",
    "    return Min,Mout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme d'Hogwild parallélisé avec pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous commençons par simplement paralléliser l'algorithme précédant (en particulier la fonction hog_loop). Les autres contributions de l'article seront developpées dans un second temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quent\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:110: UserWarning: The CUDA compiler succeeded, but said the following:\n",
      "kernel.cu\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "\n",
    "#include <time.h>\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <curand.h>\n",
    "#include <math.h>\n",
    "\n",
    "__device__ void loop(float *Min, float *Mout, float alpha, int *Nwin, int wout, int V, int d, int N, int negative, int random){\n",
    "    \n",
    "    /*\n",
    "    Une boucle permettant la mise à jour des poids du réseau à partir d'un mot et de son contexte\n",
    "    \n",
    "    float *Min : La matrice des poids du réseau de neurones reliant la couche d'input à la couche cachée (taille V*d)\n",
    "    float *Mout : La matrice des poids du réseau de neurones reliant la couche cachée à la couche d'output (taille V*d)\n",
    "    float alpha : learning rate\n",
    "    int *Nwin : contexte de wout\n",
    "    int wout : mot mis à jour\n",
    "    int V : taille du vocabulaire (nombre de mots différents)\n",
    "    int d : taille de l'espace de projection\n",
    "    int N : taille du contexte utilisé pour l'apprentissage (de chaque coté donc 2*N en tout)\n",
    "    int negative : taille du negative sampling\n",
    "    int random : un entier aléatoire (base pour construire le negative sampling)\n",
    "    */\n",
    "\n",
    "    /* Initialisation des variables */\n",
    "    float* temp;\n",
    "    temp = (float *)malloc(sizeof(float)*d);\n",
    "    int label;\n",
    "    int target_word;\n",
    "    float inn;\n",
    "    float err;\n",
    "    int r;\n",
    "    \n",
    "    /* Boucle principale sur les 2*N inputs*/\n",
    "    for(int i=0;i<2*N;i++){\n",
    "        int input_word = Nwin[i];\n",
    "        for(int j=0;j<d;j++){\n",
    "            temp[j]=0;\n",
    "        }\n",
    "        for(int k=0; k<negative+1;k++){\n",
    "            if (k==0){\n",
    "                target_word = wout;\n",
    "                label = 1;\n",
    "            } else {\n",
    "                /* negative sampling  (avec un pseudo-random generator)*/\n",
    "                r = (1664525*r+1013904223) % 4294967296;\n",
    "                target_word = r%V;\n",
    "                label = 0;\n",
    "            }\n",
    "            inn = 0;\n",
    "            for(int j=0;j<d;j++){\n",
    "                inn = inn + Min[input_word*d+j]*Mout[target_word*d+j];\n",
    "            }\n",
    "            err = label-(1/(1+exp(-inn)));\n",
    "            for(int j=0;j<d;j++){\n",
    "                temp[j] = temp[j]+err*Mout[target_word*d+j];\n",
    "            }\n",
    "            for(int j=0;j<d;j++){\n",
    "                /* MAJ Mout */\n",
    "                Mout[target_word*d+j] = Mout[target_word*d+j]+alpha*err*Min[input_word*d+j];\n",
    "            }  \n",
    "        }\n",
    "        for(int j=0;j<d;j++){\n",
    "            /* MAJ Min */\n",
    "            Min[input_word*d+j] = Min[input_word*d+j]+alpha*temp[j]; \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    free(temp);\n",
    "}\n",
    "\n",
    "\n",
    "__global__ void parallel(float *Min, float* Mout, int random, int* cst_int, float* cst_float, int* targets, int* contexts) {\n",
    "\n",
    "    /* Fonction paralélisée\n",
    "    float *Min : La matrice des poids du réseau de neurones reliant la couche d'input à la couche cachée (taille V*d)\n",
    "    float *Mout : La matrice des poids du réseau de neurones reliant la couche cachée à la couche d'output (taille V*d)\n",
    "    int* random : un entier aléatoire généré hors du device (utile pour la construction du negative sampling)\n",
    "    int* cst_int : les paramètres (int) du modèle (V,d,negative,N)\n",
    "    float* cst_float : les paramètres (float) du modèle (alpha)\n",
    "    int* targets : les mots de la base d'entraînement\n",
    "    int* context : le contexte des mots de la base d'entraînement\n",
    "    */\n",
    "    \n",
    "    int V = cst_int[0];\n",
    "    int d = cst_int[1];\n",
    "    int negative = cst_int[2];\n",
    "    int N = cst_int[3];\n",
    "\n",
    "    float alpha = cst_float[0];\n",
    "    \n",
    "    /* L'index correspond au thread et indique simplement le wout sur lequel on travaille */\n",
    "    int index = threadIdx.x; \n",
    "    int wout = targets[index];\n",
    "    \n",
    "    /* Contexte de wout */\n",
    "    int *Nwin;\n",
    "    Nwin = (int *)malloc(sizeof(int)*N*2);\n",
    "    for (int i=0;i<2*N;i++){\n",
    "        Nwin[i] = contexts[index*2*N+i];\n",
    "    }\n",
    "    \n",
    "    random = random+index;\n",
    "    \n",
    "    loop(Min,Mout,alpha,Nwin,wout,V,d,N,negative,random);\n",
    "    \n",
    "    free(Nwin);\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme d'Hogwild amélioré et parallélisé avec pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "L'amélioration consiste à rendre matriciel les opérations de mise à jour via une astuce de mutualisation du negative sampling sur tout le contexte du mot étudié.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quent\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:236: UserWarning: The CUDA compiler succeeded, but said the following:\n",
      "kernel.cu\r\n",
      "kernel.cu(22): warning: variable \"lB\" was declared but never referenced\r\n",
      "\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod_2 = SourceModule(\"\"\"\n",
    "\n",
    "\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <curand.h>\n",
    "#include <math.h>\n",
    "\n",
    "\n",
    "__device__ void multiplication(float* A, float* B, float* C, int* dim){\n",
    "    \n",
    "    /* Multplication matricielle : calcule C = A*B en supposant la multiplication licite\n",
    "    float* A\n",
    "    float* B\n",
    "    float* C \n",
    "    int* dim : contient les dimensions des matrices A et B\n",
    "    */\n",
    "    \n",
    "    int lA = dim[0];\n",
    "    int cA = dim[1];\n",
    "    int lB = dim[2];\n",
    "    int cB = dim[3];\n",
    "        \n",
    "    for(int i=0; i<lA ; i++){\n",
    "        for(int j=0; j<cB ; j++){\n",
    "            C[i*cB+j] = 0;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for(int i=0; i<lA ; i++){\n",
    "        for(int j=0; j<cB ; j++){\n",
    "            for(int k=0 ; k<cA ; k++){\n",
    "                C[i*cB+j] = C[i*cB+j] + A[i*cA+k]*B[k*cB+j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "__device__ void transpose(float* A, float* TA, int* dim){\n",
    "    /* Transposition matricielle : calcule TA = transpose(A)\n",
    "    float* A\n",
    "    float* TA\n",
    "    int* dim : contient les dimensions de la matrice A\n",
    "    */\n",
    "    \n",
    "    int lA = dim[0];\n",
    "    int cA = dim[1];\n",
    "      \n",
    "    for(int i=0; i<cA ; i++){\n",
    "        for(int j=0; j<lA ; j++){\n",
    "            TA[i*lA+j] = A[j*cA+i];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "__device__ void loop(float *Min, float *Mout, float alpha, int *Nwin, int wout, int V, int d, int N, int negative, int r){\n",
    "    /*\n",
    "    Une boucle (améliorée) permettant la mise à jour des poids du réseau à partir d'un mot et de son contexte\n",
    "    \n",
    "    float *Min : La matrice des poids du réseau de neurones reliant la couche d'input à la couche cachée (taille V*d)\n",
    "    float *Mout : La matrice des poids du réseau de neurones reliant la couche cachée à la couche d'output (taille V*d)\n",
    "    float alpha : learning rate\n",
    "    int *Nwin : contexte de wout\n",
    "    int wout : mot mis à jour\n",
    "    int V : taille du vocabulaire (nombre de mots différents)\n",
    "    int d : taille de l'espace de projection\n",
    "    int N : taille du contexte utilisé pour l'apprentissage (de chaque coté donc 2*N en tout)\n",
    "    int negative : taille du negative sampling\n",
    "    int r : un entier aléatoire (base pour construire le negative sampling)\n",
    "    */\n",
    "\n",
    "\n",
    "    /* Calcul des target_words (wout et negative sample) */\n",
    "    int *target_words;\n",
    "    target_words = (int *)malloc(sizeof(int)*(negative+1));\n",
    "    for(int i=0; i<negative+1;i++){\n",
    "        if (i==0){\n",
    "            target_words[i] = wout;\n",
    "        } else {\n",
    "            r = (1664525*r+1013904223) % 4294967296;\n",
    "            target_words[i] = r%V;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /* Matrice des labels */\n",
    "    float *label;\n",
    "    label = (float *)malloc(sizeof(float)*(2*N*(negative+1)));\n",
    "    for (int i=0; i<2*N ; i++){\n",
    "        for (int j=0 ; j<negative+1;j++){\n",
    "            if (j==0){\n",
    "                label[i*(negative+1)+j] = 1;\n",
    "            } else {\n",
    "                label[i*(negative+1)+j] = 0;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /* sub_Min les lignes de Min qui vont être utiles */\n",
    "    float* sub_Min;\n",
    "    sub_Min = (float *)malloc(sizeof(float)*2*N*d);\n",
    "    for (int i=0; i<2*N ; i++){\n",
    "        for (int j=0 ; j<d ; j++){\n",
    "            sub_Min[i*d+j] = Min[Nwin[i]*d+j];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /* sub_Mout les lignes de Mout qui vont être utiles */\n",
    "    float* sub_Mout;\n",
    "    sub_Mout = (float *)malloc(sizeof(float)*(negative+1)*d);\n",
    "    for (int i=0; i<negative+1 ; i++){\n",
    "        for (int j=0 ; j<d ; j++){\n",
    "            sub_Mout[i*d+j] = Mout[target_words[i]*d+j];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /* sub_Mout_transpose */\n",
    "    float* sub_Mout_transpose;\n",
    "    sub_Mout_transpose = (float *)malloc(sizeof(float)*d*(negative+1));\n",
    "    int dim_sub_Mout[2];\n",
    "    dim_sub_Mout[0] = negative+1;\n",
    "    dim_sub_Mout[1] = d;\n",
    "    transpose(sub_Mout, sub_Mout_transpose, dim_sub_Mout);\n",
    "    \n",
    "    /* INN */\n",
    "    float* INN;\n",
    "    INN = (float *)malloc(sizeof(float)*2*N*(negative+1));\n",
    "    int dim[4];\n",
    "    dim[0] = 2*N;\n",
    "    dim[1] = d;\n",
    "    dim[2] = d;\n",
    "    dim[3] = negative+1;\n",
    "    multiplication(sub_Min, sub_Mout_transpose, INN, dim);\n",
    "    \n",
    "    /* ERR */\n",
    "    float* ERR;\n",
    "    ERR = (float *)malloc(sizeof(float)*2*N*(negative+1));\n",
    "    for (int i=0; i<2*N ; i++){\n",
    "        for (int j=0 ; j<negative+1;j++){\n",
    "            ERR[i*(negative+1)+j] = label[i*(negative+1)+j] - (1/(1+exp(-1*INN[i*(negative+1)+j])));\n",
    "        }\n",
    "    }  \n",
    "    \n",
    "    /* TEMP */\n",
    "    float* TEMP;\n",
    "    TEMP = (float *)malloc(sizeof(float)*2*N*d);\n",
    "    dim[0] = 2*N;\n",
    "    dim[1] = negative+1;\n",
    "    dim[2] = negative+1;\n",
    "    dim[3] = d;\n",
    "    multiplication(ERR, sub_Mout, TEMP, dim);\n",
    "    \n",
    "    /* MAJ Mout */\n",
    "    float* ERR_transpose_alpha;\n",
    "    ERR_transpose_alpha = (float *)malloc(sizeof(float)*(negative+1)*2*N);\n",
    "    for (int i=0; i<negative+1 ; i++){\n",
    "        for (int j=0 ; j<2*N;j++){\n",
    "            ERR_transpose_alpha[i*2*N+j] = alpha*ERR[j*(negative+1)+i]; \n",
    "        }\n",
    "    }\n",
    "    float* MAJ_Mout;\n",
    "    MAJ_Mout = (float *)malloc(sizeof(float)*(negative+1)*d);\n",
    "    dim[0] = negative+1;\n",
    "    dim[1] = 2*N;\n",
    "    dim[2] = 2*N;\n",
    "    dim[3] = d;\n",
    "    multiplication(ERR_transpose_alpha, sub_Min, MAJ_Mout, dim);\n",
    "    for (int i=0; i<negative+1 ; i++){\n",
    "        for (int j=0 ; j<d;j++){\n",
    "            Mout[target_words[i]*d+j] = Mout[target_words[i]*d+j] + MAJ_Mout[i*d+j]; \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /* MAJ Min */\n",
    "    for(int i = 0;i<2*N;i++){\n",
    "        for(int j = 0;j<d;j++){\n",
    "            Min[Nwin[i]*d+j] = Min[Nwin[i]*d+j] + alpha*TEMP[i*d+j];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    free(target_words);\n",
    "    free(label);\n",
    "    free(sub_Min);\n",
    "    free(sub_Mout);\n",
    "    free(sub_Mout_transpose);\n",
    "    free(INN);\n",
    "    free(ERR);\n",
    "    free(TEMP);\n",
    "    free(ERR_transpose_alpha);\n",
    "    free(MAJ_Mout);\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "__global__ void parallel(float *Min, float* Mout, int random, int* cst_int, float* cst_float, int* targets, int* contexts) {\n",
    "    \n",
    "    /* Fonction paralélisée\n",
    "    float *Min : La matrice des poids du réseau de neurones reliant la couche d'input à la couche cachée (taille V*d)\n",
    "    float *Mout : La matrice des poids du réseau de neurones reliant la couche cachée à la couche d'output (taille V*d)\n",
    "    int* random : un entier aléatoire généré hors du device (utile pour la construction du negative sampling)\n",
    "    int* cst_int : les paramètres (int) du modèle (V,d,negative,N)\n",
    "    float* cst_float : les paramètres (float) du modèle (alpha)\n",
    "    int* targets : les mots de la base d'entraînement\n",
    "    int* context : le contexte des mots de la base d'entraînement\n",
    "    */\n",
    "    \n",
    "    int V = cst_int[0];\n",
    "    int d = cst_int[1];\n",
    "    int negative = cst_int[2];\n",
    "    int N = cst_int[3];\n",
    "\n",
    "    float alpha = cst_float[0];\n",
    "    \n",
    "    /*l'index correspond au thread et indique simplement le wout sur lequel on travaille*/\n",
    "    int index = threadIdx.x; \n",
    "    int wout = targets[index];\n",
    "    \n",
    "    /*Context de wout*/\n",
    "    int *Nwin;\n",
    "    Nwin = (int *)malloc(sizeof(int)*N*2);\n",
    "    for (int i=0;i<2*N;i++){\n",
    "        Nwin[i] = contexts[index*2*N+i];\n",
    "    }\n",
    "    \n",
    "    random = random+index;\n",
    "    \n",
    "    loop(Min,Mout,alpha,Nwin,wout,V,d,N,negative,random);\n",
    "   \n",
    "    free(Nwin);\n",
    "    \n",
    "}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement avec Pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec_parallel(X, params, func):\n",
    "    \"\"\" fonction d'entraînement du reseau de neurones avec pycuda\n",
    "    X (array) : texte constituant la base d'entraînement\n",
    "    params (dict) :  N, n_epochs, alpha, negative, d\n",
    "    N (int) : taille du contexte d'un mot \n",
    "    n_epochs (int) : nombre d'entraînement complet du réseau\n",
    "    alpha (float) : learning rate\n",
    "    negative (int) : taille du negative sampling\n",
    "    d (int) : taille de l'espace de représentation\n",
    "    n_parallel (int) : nombre de threads utilisés pour la parallélisation\n",
    "    func (fonction C) : la fonction à paralléliser\n",
    "    \n",
    "    return : Min, Mout (array) les matrices de poids du réseau entraîné\n",
    "    \"\"\"\n",
    "    \n",
    "    N = params['N']\n",
    "    n_epochs = params['n_epochs']\n",
    "    alpha = params['alpha']\n",
    "    negative = params['negative']\n",
    "    d = params['d']\n",
    "    n_parallel = params['n_parallel']\n",
    "    \n",
    "    V = np.max(X)+1\n",
    "    n_words = np.shape(X)[0]\n",
    "    \n",
    "    #Paramètres du modèle\n",
    "    cst_int = np.array([V,d,negative,N])\n",
    "    cst_float = np.array([alpha])\n",
    "    \n",
    "    cst_int = cst_int.astype(np.int32) #Pycuda ne peut travailler qu'avec un format 32 bits\n",
    "    cst_float = cst_float.astype(np.float32)\n",
    "    \n",
    "    #Initialisation de Min et Mout\n",
    "    Min = np.array([rd.random() for i in range(V*d)]).reshape((V,d))\n",
    "    Mout = np.array([rd.random() for i in range(V*d)]).reshape((V,d))\n",
    "    \n",
    "    Min = Min.astype(np.float32)\n",
    "    Mout = Mout.astype(np.float32)\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        epochs_order = np.array([t for t in range(N,n_words-N)])\n",
    "        np.random.shuffle(epochs_order)\n",
    "        \n",
    "        #bloc de threads\n",
    "        for j in range(n_words//n_parallel+1):\n",
    "            \n",
    "            sub_epochs_order = epochs_order[j*n_parallel:(j+1)*n_parallel]\n",
    "            \n",
    "            n_threads = np.shape(sub_epochs_order)[0]\n",
    "            \n",
    "            contexts=[]\n",
    "            targets=[]\n",
    "            \n",
    "            for k in sub_epochs_order : \n",
    "                targets.append(X[k])\n",
    "                Nwin=[]\n",
    "                for l in range(N):\n",
    "                    Nwin.append(X[k-N+l])\n",
    "                for l in range(N):\n",
    "                    Nwin.append(X[k+l+1])\n",
    "                contexts.append(Nwin)\n",
    "            \n",
    "            targets = np.array(targets)\n",
    "            contexts = np.array(contexts)\n",
    "            \n",
    "            targets = targets.astype(np.int32)\n",
    "            contexts = contexts.astype(np.int32)\n",
    "            \n",
    "            r = np.array([rd.randint(1,1000000)])\n",
    "            r = r.astype(np.int32)\n",
    "            \n",
    "            #Allocation de la mémoire sur le device\n",
    "            Min_gpu = cuda.mem_alloc(Min.nbytes)\n",
    "            Mout_gpu = cuda.mem_alloc(Mout.nbytes)\n",
    "            r_gpu = cuda.mem_alloc(r.nbytes)\n",
    "            cst_int_gpu = cuda.mem_alloc(cst_int.nbytes)\n",
    "            cst_float_gpu = cuda.mem_alloc(cst_float.nbytes)\n",
    "            targets_gpu = cuda.mem_alloc(targets.nbytes)\n",
    "            contexts_gpu = cuda.mem_alloc(contexts.nbytes)\n",
    "            \n",
    "            #Copie des variables sur le device\n",
    "            cuda.memcpy_htod(Min_gpu, Min)\n",
    "            cuda.memcpy_htod(Mout_gpu, Mout)\n",
    "            cuda.memcpy_htod(r_gpu, r)\n",
    "            cuda.memcpy_htod(cst_int_gpu,cst_int)\n",
    "            cuda.memcpy_htod(cst_float_gpu,cst_float)\n",
    "            cuda.memcpy_htod(contexts_gpu,contexts)\n",
    "            cuda.memcpy_htod(targets_gpu,targets)\n",
    "            \n",
    "            #Fonction parallélisée\n",
    "            func(Min_gpu, Mout_gpu, r_gpu, cst_int_gpu, cst_float_gpu, targets_gpu, contexts_gpu, block=(n_threads,1,1))\n",
    "            \n",
    "            #Récupération des variables sur le host\n",
    "            cuda.memcpy_dtoh(Min, Min_gpu)\n",
    "            cuda.memcpy_dtoh(Mout, Mout_gpu)\n",
    "\n",
    "\n",
    "    return Min,Mout\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "Nous avons effectué des essais sur différents jeux de données relativement petits (de la taille d'un article ou d'un livre par exemple). Le jeu de données mentionné dans l'article nécessite de nombreuses heures d'entraînement, avec des GPU plus performants que ceux dont nous disposions et il n'était pas envisageable (ni utile) de les réutiliser à notre échelle.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "Les jeux de données (des textes brutes) doivent être traités avant de pouvoir entraîner l'algorithme word2vec. L'idée est de commencer par nettoyer les corpus en supprimant les caractères spéciaux et les majuscules. Il faut ensuite encoder les textes via la méthode one-hot. En l'occurrence, nous avons choisi de travailler avec une variante de l'encodage one-hot. Un corpus de texte est alors représenté par un vecteur X de taille n (le nombre de mots du corpus), on donne à chaque mot un identidiant et l'élement Xi est l'identifiant du ième mot du corpus.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "Les fonctions relatives au nettoyage des données ne sont pas présentées dans ce notebook car elles sont simples et ne présentent pas d'intérêt pour cet exposé.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extrait des misérables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          e\n",
       "1    address\n",
       "2     marius\n",
       "3     turned\n",
       "4       pale\n",
       "5        all\n",
       "6        the\n",
       "7      blood\n",
       "8     flowed\n",
       "9       back\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chargement du jeu de données\n",
    "text  = open(\"data/LesMiserables_cleaned.txt\", \"r\")\n",
    "text = text.read()\n",
    "text = text.split(\" \")\n",
    "text_serie = pd.Series(text)\n",
    "text_serie.head(10)\n",
    "text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding\n",
    "text_lb_miserables = LabelEncoder()\n",
    "X_hot_miserables = text_lb_miserables.fit_transform(text_serie.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100648,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nombre de mots du texte\n",
    "np.shape(X_hot_miserables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2955,  180, 5635, ...,  303,  608,    0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_hot_miserables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10215"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Taille du vocabulaire\n",
    "np.max(X_hot_miserables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['constant']\n",
      "[9667]\n"
     ]
    }
   ],
   "source": [
    "print(text_lb_miserables.inverse_transform([1995]))\n",
    "print(text_lb_miserables.transform([\"valjean\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Article Wikipédia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         eric\n",
       "1      clapton\n",
       "2           né\n",
       "3           le\n",
       "4         mars\n",
       "5            à\n",
       "6       ripley\n",
       "7         près\n",
       "8           de\n",
       "9    guildford\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chargement du jeu de données\n",
    "text  = open(\"data/Clapton_cleaned.txt\", \"r\")\n",
    "text = text.read()\n",
    "text = text.split(\" \")\n",
    "text_serie = pd.Series(text)\n",
    "text_serie.head(10)\n",
    "text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding\n",
    "text_lb_clapton = LabelEncoder()\n",
    "X_hot_clapton = text_lb_clapton.fit_transform(text_serie.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5700,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nombre de mots du texte\n",
    "np.shape(X_hot_clapton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 583,  312, 1058, ..., 1481,  470,    0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_hot_clapton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1747"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Taille du vocabulaire\n",
    "np.max(X_hot_clapton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acoustique']\n",
      "[583]\n"
     ]
    }
   ],
   "source": [
    "print(text_lb_clapton.inverse_transform([21]))\n",
    "print(text_lb_clapton.transform([\"eric\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paramètres utilisés pour l'entraînement des modèles\n",
    "params = {\n",
    "    \"N\" : 2,\n",
    "    \"n_epochs\" : 20,\n",
    "    \"alpha\" : 10**(-3),\n",
    "    \"negative\" : 5,\n",
    "    \"d\" : 10,\n",
    "    'n_parallel' : 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevision(mot,Min,Mout,text_lb,p):\n",
    "    \"\"\" Permet d'effectuer la tâche de prédiction à partir de Min et Mout\n",
    "    mot (str) : mot à prédire\n",
    "    Min (array) : matrice des poids du réseau de neurones\n",
    "    Mout (array) : matrice des poids du réseau de neurones\n",
    "    text_lb : encodeur (pour permettre d'associer les mots à leur identifiant)\n",
    "    p : les p mots les plus probables\n",
    "    \n",
    "    return : p prédictions des mots les plus probables du contexte\n",
    "    \"\"\"\n",
    "    \n",
    "    index = text_lb.transform([mot])[0]\n",
    "    sub_Min = Min[index]\n",
    "    \n",
    "    prev = np.dot(sub_Min,Mout.T)\n",
    "    prev = np.exp(prev)/(np.sum(np.exp(prev)))\n",
    "    \n",
    "    index_prev = np.flip(np.argsort(prev))[:p]\n",
    "\n",
    "    word_prev = text_lb.inverse_transform(index_prev)\n",
    "    \n",
    "    return word_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme non parallélisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "Pour l'algorithme non parallélisé, on travaille uniquement avec le petit jeu de données, l'entraînement du second étant beaucoup trop long.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps pour effectuer 20 epochs : 85.97421360015869\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "Min,Mout = train_word2vec(X=X_hot_clapton, params=params)\n",
    "dt=time.time()-t0\n",
    "print(\"temps pour effectuer %d epochs : %s\" % (params['n_epochs'],dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de' 'clapton' 'à']\n",
      "['de' 'en' 'et']\n",
      "['de' 'l' 'le']\n",
      "['connait' 'songs' 'meurt']\n"
     ]
    }
   ],
   "source": [
    "print(prevision(\"eric\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"clapton\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"guitare\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"cocaine\",Min,Mout,text_lb_clapton,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "Un premier constat (que l'on observera également par la suite) est que l'entraînement d'un tel réseau de neurones est relativement complexe et nécessite une certaine quantité de travaille en preprocessing des données. Ici, il semble que l'algorithme ait convergé en 20 epochs mais on observe que la surabondance de certains mots ('à', 'le', 'de', 'un' etc.) a tendance à fausser les résultats d'entraînement. Il convient donc de les supprimer en supposant qu'ils ne sont pas informatifs puisque prépondérants dans les contextes de la quasi totalité des mots des corpus de texte. On observe tout de même une certaine logique dans les prédictions ici : il s'agit d'un article sur Eric Clapton et le mot \"clapton\" se situe dans le contexte du mot \"eric\", de même les mots \"songs\" et \"meurt\" se situent dans le contexte du mot \"cocaine\" ce qui semble logique. (Remarque : ces résultats peuvent varier en ré éxécutant le notebook).\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "L'autre résultat est que plus d'une minute est nécessaire à l'entraînement du réseau est nécessaire (pour effectuer 20 epochs). Observons le gain dû à la parallélisation.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hogwild parallélisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps pour effectuer 20 epochs : 1.054260015487671\n"
     ]
    }
   ],
   "source": [
    "func = mod.get_function(\"parallel\")\n",
    "t0=time.time()\n",
    "Min, Mout = train_word2vec_parallel(X=X_hot_clapton, params=params, func=func)\n",
    "dt=time.time()-t0\n",
    "print(\"temps pour effectuer %d epochs : %s\" % (params['n_epochs'],dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "Un gain de temps d'un facteur <b>80</b> est donc obtenu via l'amélioration (cela peut varier selon les éxecutions du notebook). C'est loin du facteur 1000 espéré mais l'amélioration est tout de même notable. La perte de performance (par rapport à celle espérée) est dû notamment au transit d'informations parfois lourdes entre les threads ce qui pourrait être optimisé. Aussi le code C pourrait être plus performants.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fabriquée' 'jouer' 'lieu']\n",
      "['l' 'coloris' 'marley']\n",
      "['coloris' 'qu’au' 'jouer']\n",
      "['lieu' 'fabriquée' 'japon']\n"
     ]
    }
   ],
   "source": [
    "print(prevision(\"eric\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"clapton\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"guitare\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"cocaine\",Min,Mout,text_lb_clapton,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il semble que l'algorithme n'a pas convergé. Vérifions avec plus d'epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"N\" : 2,\n",
    "    \"n_epochs\" : 25,\n",
    "    \"alpha\" : 10**(-3),\n",
    "    \"negative\" : 5,\n",
    "    \"d\" : 10,\n",
    "    'n_parallel' : 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps pour effectuer 25 epochs : 1.3588569164276123\n"
     ]
    }
   ],
   "source": [
    "func = mod.get_function(\"parallel\")\n",
    "t0=time.time()\n",
    "Min, Mout = train_word2vec_parallel(X=X_hot_clapton, params=params, func=func)\n",
    "dt=time.time()-t0\n",
    "print(\"temps pour effectuer %d epochs : %s\" % (params['n_epochs'],dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clapton' 'concurrence' 'un']\n",
      "['en' 'chanson' 'le']\n",
      "['clapton' 'chanson' 'en']\n",
      "['chanson' 'le' 'exprimera']\n"
     ]
    }
   ],
   "source": [
    "print(prevision(\"eric\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"clapton\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"guitare\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"cocaine\",Min,Mout,text_lb_clapton,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fois ci les mots prédits semblent plus cohérents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hogwild parallélisé et amélioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"N\" : 2,\n",
    "    \"n_epochs\" : 20,\n",
    "    \"alpha\" : 10**(-3),\n",
    "    \"negative\" : 5,\n",
    "    \"d\" : 10,\n",
    "    'n_parallel' : 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps pour effectuer 20 epochs : 3.8932995796203613\n"
     ]
    }
   ],
   "source": [
    "func_2 = mod_2.get_function(\"parallel\")\n",
    "t0=time.time()\n",
    "Min, Mout = train_word2vec_parallel(X=X_hot_clapton, params=params, func=func_2)\n",
    "dt=time.time()-t0\n",
    "print(\"temps pour effectuer %d epochs : %s\" % (params['n_epochs'],dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "On observe malheureusement une perte de performance par rapport à la version seulement parallélisée de l'algorithme. Le gain de temps n'est plus que d'un rapport <b>25</b>. En fait c'est normal. Le code a été modifié pour que les opérations soient légérement plus nombreuses mais qu'elles soient majoritairement des produits entre matrices et non plus des produits entre vecteurs. L'idée est ensuite de jouer sur le fait que le produit matriciel peut-être optimisé en utilisant des librairies d'algèbre linéaire. Malheureusement, nous n'avons pas réussi à utiliser ces librairies via pycuda et nous avons programmer notre propre produit matriciel (très peu optimisé et très gourmand). D'après l'article l'utilisation de tels librairies permet de surpasser les implémentations de word2vec les plus rapides par ailleurs puisqu'avec leur matériel (bien plus puissants que nos ordinateurs), plusieurs centaines de millions de mots peuvent être appris par secondes (voir l'article).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clapton' 'ces' 'lorsqu']\n",
      "['anciens' 'un' 'ironique']\n",
      "['quitte' 'actifs' 'un']\n",
      "['donnent' 'jones' 'porter']\n"
     ]
    }
   ],
   "source": [
    "print(prevision(\"eric\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"clapton\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"guitare\",Min,Mout,text_lb_clapton,3))\n",
    "print(prevision(\"cocaine\",Min,Mout,text_lb_clapton,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du temps de calcul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adapté à son GPU\n",
    "- librairies d'algèbres\n",
    "- difficultées : C (débutant), utilisation et installation de pycuda (assez complexe sur nos postes) et débogage difficile, programmation GPU, algorithme amélioré non présenté dans l'article (seulement l'idée de l'implémentation)\n",
    "- intéressant, aller plus loin\n",
    "- plus optimiser le code C\n",
    "- travailler d'avantage sur les données\n",
    "- observation tout de même de gains importants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMQ Dupre : \n",
    "\n",
    "regarder \n",
    "// ++i i++\n",
    "   for(auto it: temp)\n",
    "       *it = 0;\n",
    "    \n",
    "for(float* it = temp; it != ; ++it)\n",
    "      *it = 0;\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
