{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Element Logiciel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autour de l'article, présentation du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "L'idée du projet est de paralléliser l'algorithme word2vec. On s'appuie pour cela sur l'article de Ji, Shihao, et al. <i>Parallelizing word2vec in shared and distributed memory</i>. Nous avons utilisé <i> pycuda </i> pour effectuer la programmation GPU. Dans ce notebook on utilise donc le langage Python comme langage d'environnement : on s'en sert principalement pour préparer les données, présenter les résultats et exécuter les fonctions à paralléliser. L'algorithme word2vec est en revanche codé en C.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "L'algorithme word2vec permet de représenter les mots d'un texte dans un espace de petite dimension (typiquement R<sup>d</sup> avec d de l'ordre de 10). Il permet donc de passer d'une représentation des données sous forme one-hot (où la taille de l'espace des données est égale à la taille du vocabulaire du corpus de texte étudié) à une représentaion dans R<sup>d</sup>. L'idée est que, des mots proches sémantiquement (on les suppose proches si ils sont proches dans le texte) doivent être proches dans l'espace de projection R<sup>d</sup> (au sens euclidien cette fois).\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "L'architecture de word2vec est un réseau de neurones à une couche cachée. Si l'on souhaite que notre espace de projection soit R<sup>d</sup>, on choisira d neurones dans la couche cachée. Les couches d'entrée et de sortie sont de taille V où V est la taille du vocabulaire du corpus de texte étudié. Pour construire notre matrice de projection dans R<sup>d</sup>, on entraîne ce réseau de neurones via une tâche de prédiction annexe. Pour un mot en entrée, le réseau doit prédire le où les mots les plus proches sémantiquement. La matrice des poids de passage de la couche d'input à la couche cachée sera alors notre matrice de projection. On construit donc un ensemble d'entraînement (X,Y) où on fixe la taille N du contexte utilisé et les couples (Xi,Yi) sont determinés de la manière suivante. Pour un mot Xi du texte et pour les mots Yi1, Yi2,...,YiN qui sont autour du mot Xi dans le texte les couples (Xi,Yi1),...(Xi,YiN) forment la base d'entraînement.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "L'entraînement de ce réseau nécessite l'implémentation de l'algorithme de descente de gradient. L'article met en avant les inconvénients de cet algorithme dans ce cas précis, en particulier l'évaluation de la fonction de perte et de ses gradients est très gourmande (le nombre de calculs étant proportionnel au nombre de mots dans le texte). On préférera donc utiliser la méthode de descente de gradient stochastique (avec negative sampling) qui n'utilise qu'une faible proportion des mots du texte à chaque évaluation des oracles. A chaque étape la méthode de gradient stochastique met à jour les poids correspondant à un mot Xi et à un mot de son contexte Yij. Le negative sampling consiste à prendre des mots au hasard dans le texte à chaque étape et à mettre à jour les poids correspondant en considérant que ces mots ne sont pas dans le contexte de Xi.\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "Dans l'article, les auteurs proposent, pour accélerer l'implémentation de la descente de gradient stochastique, de paralléliser les boucles de mise à jour des poids du réseau (voir section III de l'article). Cela risque de créer des conflits (puisque les mêmes poids risquent d'être mis à jour au même moment) mais on s'en affranchit, cela peut réduire théoriquement les taux de convergence de l'algorithme mais permet de gagner en vitesse (d'un facteur du nombre de parallélisation). On observe que pour des grands textes et à condition de ne pas utiliser un nombre de threads trop élevé, les conflits sont rares et n'empechent pas la convergence. L'algorihme que l'on parallélise est l'implémentation d'Hogwild (voir <i>Algorithm 1</i> de l'article).\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:justify\">\n",
    "La deuxième amélioration proposée dans l'article est double. Elle consiste à exploiter plus encore les propriétés locales des informations utilisées à chaque boucle de mise à jour. L'idée est de partager le <i>negative sampling</i> entre les mots d'un même contexte. Cela va permettre de passer des opérations de produits scalaires (opérations BLAS-1)  utilisés massivement dans l'implémentation d'Hogwild à des produits matriciels (opérations BLAS-3) en théorie plus efficaces en utilisant des bibliothèques performatantes d'algèbre linéaire.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "On s'attache dans ce projet à mettre en place ces deux techniques sur un petit jeu de données et d'observer les gains effectivement obtenus.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "On utilise comme jeu de données, un extrait des Misérables de Victor Hugo traduit en anglais. Le texte a été nétoyé de tous ces caractères spéciaux. Cet extrait contient 39.079 mots dont 6128 mots différents. On commence par encoder le texte sous forme one-hot : matrice de taille n*V où n est le nombre de mots du texte et V le nombre de mots différents et l'élément (i,j) de cette matrice vaut 1 si le ième mot est identifié comme le jème mot du vocabulaire, 0 sinon. Cette matrice est notre base de travail pour l'entraînement du réseau de neurones. Nous avons trouvé plus pratique de travailler avec une forme équivalente : un vecteur de taille n où le ième terme est l'identifiant (un entier entre 1 et V) du ième mot du texte.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          e\n",
       "1    address\n",
       "2     marius\n",
       "3     turned\n",
       "4       pale\n",
       "5        all\n",
       "6        the\n",
       "7      blood\n",
       "8     flowed\n",
       "9       back\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chargement du jeu de données\n",
    "text  = open(\"data/LesMiserables_cleaned.txt\", \"r\")\n",
    "text = text.read()\n",
    "text = text.split(\" \")\n",
    "text_serie = pd.Series(text)\n",
    "text_serie.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "text_lb = LabelEncoder()\n",
    "X_hot = text_lb.fit_transform(text_serie.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100648,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_hot)\n",
    "#Le ième mot du texte est le mot j => X_hot_ij = 1\n",
    "#dimension de X_hot : n*V\n",
    "#n : nombre de mot dans le texte\n",
    "#V : nombre de mots différents dans le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2955,  180, 5635, ...,  303,  608,    0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10215"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(X_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1776']\n",
      "[5317]\n"
     ]
    }
   ],
   "source": [
    "print(text_lb.inverse_transform([10]))\n",
    "print(text_lb.transform([\"let\"]))\n",
    "#Il est plus simple de représenter le texte sous cette forme\n",
    "#X_i est l'identifiant du ième mot du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hogwild algorithm in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify\">\n",
    "On commence par coder l'algorithme sous Python. Cette première implémentation de la méthode ne sera donc parallélisée et nous permettra de comparer les résultats et la vitesse de calcul avec le code parallélisé sous pycuda. On rédige volontairemnt un code non utilisé on utilise pas par exemple les bibliothèques de calcul d'algèbre linéaire pour le calcul matriciel. Cela nous permettra de comparer les méthodes sous la même base de travail et d'observer correctement les gains engendrés par les améliorations de l'article.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On commence par rédiger le code sans parallélisation \n",
    "def sig(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def hog_loop(Min,Mout,alpha,wout,Nwin,negative):\n",
    "    #Min matrice dans V*d\n",
    "    #Mout matrice dans V*d\n",
    "    #wout un élément de 0:V-1\n",
    "    #Nwin n entiers de 0:V-1\n",
    "    V,d = Min.shape\n",
    "    N = Nwin.shape[0]\n",
    "    for i in range(N):\n",
    "        input_word = Nwin[i]\n",
    "        temp = np.array([0.0]*d)\n",
    "        for k in range(negative+1):\n",
    "            if k == 0:\n",
    "                target_word = wout\n",
    "                label = 1\n",
    "            else : \n",
    "                target_word = rd.randint(0,V-1)\n",
    "                label = 0\n",
    "            inn = 0\n",
    "            for j in range(d):\n",
    "                inn = inn + Min[input_word,j]*Mout[target_word,j]\n",
    "            err = label - sig(inn)\n",
    "            for j in range(d):\n",
    "                temp[j] = temp[j] + err*Mout[target_word,j]\n",
    "            for j in range(d):\n",
    "                Mout[target_word,j] = Mout[target_word,j] + alpha*err*Min[input_word,j]\n",
    "        for j in range(d):\n",
    "            Min[input_word,j] = Min[input_word,j] + alpha*temp[j]\n",
    "    return(Min,Mout)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(X,params):\n",
    "    \n",
    "    N = params['N']\n",
    "    n_epochs = params['n_epochs']\n",
    "    alpha = params['alpha']\n",
    "    negative = params['negative']\n",
    "    d = params['d']\n",
    "    \n",
    "    V = np.max(X)+1\n",
    "    n_words = np.shape(X)[0]\n",
    "    \n",
    "    Min = np.array([rd.random() for i in range(V*d)]).reshape((V,d))\n",
    "    Mout = np.array([rd.random() for i in range(V*d)]).reshape((V,d))\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        epochs_order = np.array([t for t in range(N,n_words-N)])\n",
    "        np.random.shuffle(epochs_order)\n",
    "        \n",
    "        for j in epochs_order:\n",
    "            \n",
    "            wout = X[j]\n",
    "            \n",
    "            Nwin = []\n",
    "            for k in range(N):\n",
    "                Nwin.append(X[j-N+k])\n",
    "            for k in range(N):\n",
    "                Nwin.append(X[j+k+1])\n",
    "            Nwin = np.array(Nwin)\n",
    "                \n",
    "            Min, Mout = hog_loop(Min,Mout,alpha,wout,Nwin,negative)\n",
    "            \n",
    "    \n",
    "    return Min,Mout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"N\" : 2,\n",
    "    \"n_epochs\" : 10,\n",
    "    \"alpha\" : 10**(-3),\n",
    "    \"negative\" : 4,\n",
    "    \"d\" : 10\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=time.time()\n",
    "Min,Mout = train_word2vec(X=X_hot, params=params)\n",
    "t=time.time()-t\n",
    "print(\"temps pour effectuer %d epochs : %s\" % (params['n_epochs'],t))\n",
    "print(Min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevision(mot,Min,Mout,text_lb,p):\n",
    "    \n",
    "    index = text_lb.transform([mot])[0]\n",
    "    sub_Min = Min[index]\n",
    "    \n",
    "    prev = np.dot(sub_Min,Mout.T)\n",
    "    prev = np.exp(prev)/(np.sum(np.exp(prev)))\n",
    "    \n",
    "    index_prev = np.flip(np.argsort(prev))[:p]\n",
    "\n",
    "    word_prev = text_lb.inverse_transform(index_prev)\n",
    "    \n",
    "    return word_prev\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prevision(\"let\",Min,Mout,text_lb,4))\n",
    "print(prevision(\"it\",Min,Mout,text_lb,4))\n",
    "print(prevision(\"be\",Min,Mout,text_lb,4))\n",
    "print(prevision(\"wisdom\",Min,Mout,text_lb,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in Pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMQ Dupre : \n",
    "\n",
    "regarder \n",
    "// ++i i++\n",
    "   for(auto it: temp)\n",
    "       *it = 0;\n",
    "    \n",
    "for(float* it = temp; it != ; ++it)\n",
    "      *it = 0;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "\n",
    "#include <time.h>\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <curand.h>\n",
    "#include <math.h>\n",
    "\n",
    "__device__ void loop(float *Min, float *Mout, float alpha, int *Nwin, int wout, int V, int d, int N, int negative, int random){\n",
    "    \n",
    "    /*\n",
    "    HOGWILD LOOP\n",
    "    \n",
    "    float *Min : initialisé en dehors de pycuda, poids de la PREMIERE couche du RN (attention float * et non float **) (V*d)\n",
    "    float *Mout : initialisé en dehors de pycuda, poids de la DEUXIEME couche du RN (attention float * et non float **) (V*d)\n",
    "    float alpha : learning rate\n",
    "    int *Nwin : 2*N input du contexte de l'output\n",
    "    int wout : output\n",
    "    int V : taille du vocabulaire (nombre de mots différents)\n",
    "    int d : taille de l'espace de représentation\n",
    "    int N : taille du contexte utilisé pour l'apprentissage (de chaque coté donc 2*N en tout)\n",
    "    int negative : nb de negative utilisé pour l'apprentissage\n",
    "    int random : un entier aléatoire (difficulté pour générer de l'aléat sur le device) pour les negative\n",
    "    \n",
    "    Une boucle de l'Algo 1 de l'article. Permet de mettre à jour Min et Mout pour un wout (et son contexte associé).\n",
    "    */\n",
    "\n",
    "    /* Init variables */\n",
    "    float* temp;\n",
    "    temp = (float *)malloc(sizeof(float)*d);\n",
    "    int label;\n",
    "    int target_word;\n",
    "    float inn;\n",
    "    float err;\n",
    "    int r;\n",
    "    \n",
    "    /* Boucle principale sur les 2*N inputs*/\n",
    "    for(int i=0;i<2*N;i++){\n",
    "        int input_word = Nwin[i];\n",
    "        for(int j=0;j<d;j++){\n",
    "            temp[j]=0;\n",
    "        }\n",
    "        for(int k=0; k<negative+1;k++){\n",
    "            if (k==0){\n",
    "                target_word = wout;\n",
    "                label = 1;\n",
    "            } else {\n",
    "                /* negative sampling */\n",
    "                r = (1664525*r+1013904223) % 4294967296;\n",
    "                target_word = r%V;\n",
    "                label = 0;\n",
    "            }\n",
    "            inn = 0;\n",
    "            for(int j=0;j<d;j++){\n",
    "                inn = inn + Min[input_word*d+j]*Mout[target_word*d+j];\n",
    "            }\n",
    "            err = label-(1/(1+exp(-inn)));\n",
    "            for(int j=0;j<d;j++){\n",
    "                temp[j] = temp[j]+err*Mout[target_word*d+j];\n",
    "            }\n",
    "            for(int j=0;j<d;j++){\n",
    "                Mout[target_word*d+j] = Mout[target_word*d+j]+alpha*err*Min[input_word*d+j];\n",
    "            }  \n",
    "        }\n",
    "        for(int j=0;j<d;j++){\n",
    "            Min[input_word*d+j] = Min[input_word*d+j]+alpha*temp[j]; \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    free(temp);\n",
    "}\n",
    "\n",
    "\n",
    "__global__ void parallel(float *Min, float* Mout, int random, int* cst_int, float* cst_float, int* targets, int* contexts) {\n",
    "\n",
    "    /*\n",
    "    Parallélisation\n",
    "    float *Min : Min initialisé\n",
    "    float *Mout : Mout initialisé\n",
    "    int* random : un entier aléatoire généré hors du device\n",
    "    int* cst_int : les constantes entières utiles (V,d,negative,N)\n",
    "    float* cst_float : les constantes float utiles (alpha)\n",
    "    int* targets : on va travailler sur wout=targets[idx]\n",
    "    int* context : le contexte associé sera contexts[idx]\n",
    "    */\n",
    "\n",
    "    /*préparation des paramètres pour la fonction loop*/\n",
    "    \n",
    "    \n",
    "    /*constantes entières passés depuis le code python*/\n",
    "    int V = cst_int[0];\n",
    "    int d = cst_int[1];\n",
    "    int negative = cst_int[2];\n",
    "    int N = cst_int[3];\n",
    "\n",
    "    /*constante float passé depuis python*/\n",
    "    float alpha = cst_float[0];\n",
    "    \n",
    "    /*l'index correspond au thread et indique simplement le wout sur lequel on travaille*/\n",
    "    int index = threadIdx.x; \n",
    "    int wout = targets[index];\n",
    "    \n",
    "    /*Context de wout*/\n",
    "    int *Nwin;\n",
    "    Nwin = (int *)malloc(sizeof(int)*N*2);\n",
    "    for (int i=0;i<2*N;i++){\n",
    "        Nwin[i] = contexts[index*2*N+i];\n",
    "    }\n",
    "    \n",
    "    random = random+index;\n",
    "    \n",
    "    loop(Min,Mout,alpha,Nwin,wout,V,d,N,negative,random);\n",
    "    \n",
    "    free(Nwin);\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec_parallel(X, params, func):\n",
    "    N = params['N']\n",
    "    n_epochs = params['n_epochs']\n",
    "    alpha = params['alpha']\n",
    "    negative = params['negative']\n",
    "    d = params['d']\n",
    "    \n",
    "    V = np.max(X)+1\n",
    "    n_words = np.shape(X)[0]\n",
    "    \n",
    "    cst_int = np.array([V,d,negative,N])\n",
    "    cst_float = np.array([alpha])\n",
    "    \n",
    "    cst_int = cst_int.astype(np.int32)\n",
    "    cst_float = cst_float.astype(np.float32)\n",
    "    \n",
    "    Min = np.array([rd.random() for i in range(V*d)]).reshape((V,d))\n",
    "    Mout = np.array([rd.random() for i in range(V*d)]).reshape((V,d))\n",
    "    \n",
    "    Min = Min.astype(np.float32)\n",
    "    Mout = Mout.astype(np.float32)\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        epochs_order = np.array([t for t in range(N,n_words-N)])\n",
    "        np.random.shuffle(epochs_order)\n",
    "        \n",
    "        for j in range(n_words//1000+1):\n",
    "            sub_epochs_order = epochs_order[j*1000:(j+1)*1000]\n",
    "            \n",
    "            n_threads = np.shape(sub_epochs_order)[0]\n",
    "            \n",
    "            contexts=[]\n",
    "            targets=[]\n",
    "            \n",
    "            for k in sub_epochs_order : \n",
    "                targets.append(X[k])\n",
    "                Nwin=[]\n",
    "                for l in range(N):\n",
    "                    Nwin.append(X[k-N+l])\n",
    "                for l in range(N):\n",
    "                    Nwin.append(X[k+l+1])\n",
    "                contexts.append(Nwin)\n",
    "            \n",
    "            targets = np.array(targets)\n",
    "            contexts = np.array(contexts)\n",
    "            \n",
    "            targets = targets.astype(np.int32)\n",
    "            contexts = contexts.astype(np.int32)\n",
    "            \n",
    "            r = np.array([rd.randint(1,1000000)])\n",
    "            r = r.astype(np.int32)\n",
    "            \n",
    "            Min_gpu = cuda.mem_alloc(Min.nbytes)\n",
    "            Mout_gpu = cuda.mem_alloc(Mout.nbytes)\n",
    "            r_gpu = cuda.mem_alloc(r.nbytes)\n",
    "            cst_int_gpu = cuda.mem_alloc(cst_int.nbytes)\n",
    "            cst_float_gpu = cuda.mem_alloc(cst_float.nbytes)\n",
    "            targets_gpu = cuda.mem_alloc(targets.nbytes)\n",
    "            contexts_gpu = cuda.mem_alloc(contexts.nbytes)\n",
    "\n",
    "            cuda.memcpy_htod(Min_gpu, Min)\n",
    "            cuda.memcpy_htod(Mout_gpu, Mout)\n",
    "            cuda.memcpy_htod(r_gpu, r)\n",
    "            cuda.memcpy_htod(cst_int_gpu,cst_int)\n",
    "            cuda.memcpy_htod(cst_float_gpu,cst_float)\n",
    "            cuda.memcpy_htod(contexts_gpu,contexts)\n",
    "            cuda.memcpy_htod(targets_gpu,targets)\n",
    "            \n",
    "            func(Min_gpu, Mout_gpu, r_gpu, cst_int_gpu, cst_float_gpu, targets_gpu, contexts_gpu, block=(n_threads,1,1))\n",
    "            \n",
    "            cuda.memcpy_dtoh(Min, Min_gpu)\n",
    "            cuda.memcpy_dtoh(Mout, Mout_gpu)\n",
    "\n",
    "\n",
    "    \n",
    "    return Min,Mout\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = mod.get_function(\"parallel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"N\" : 2,\n",
    "    \"n_epochs\" : 10,\n",
    "    \"alpha\" : 10**(-3),\n",
    "    \"negative\" : 4,\n",
    "    \"d\" : 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps d'execution de 10 epochs\n",
      "11.935495853424072\n",
      "[[0.99326545 0.4685385  0.47332487 ... 0.2653461  0.8222725  0.6239358 ]\n",
      " [0.93242335 0.84805053 0.22458169 ... 0.7781303  0.39575273 0.5576866 ]\n",
      " [0.23221748 0.32908982 0.8396483  ... 0.93647116 0.6011322  0.37795612]\n",
      " ...\n",
      " [0.01812954 0.7847018  0.8054997  ... 0.5697654  0.0586516  0.09843464]\n",
      " [0.9143857  0.9028726  0.37772062 ... 0.54738826 0.61895263 0.76268846]\n",
      " [0.93561375 0.07274387 0.23634802 ... 0.4439356  0.45557424 0.6066696 ]]\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "Min, Mout = train_word2vec_parallel(X=X_hot, params=params, func=func)\n",
    "dt=time.time()-t0\n",
    "print(\"temps d'execution de %d epochs\" % params[\"n_epochs\"])\n",
    "print(dt)\n",
    "print(Min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the' 'a' 'of']\n",
      "['the' 'a' 'of']\n",
      "['the' 'a' 'to']\n"
     ]
    }
   ],
   "source": [
    "print(prevision(\"jean\",Min,Mout,text_lb,3))\n",
    "print(prevision(\"valjean\",Min,Mout,text_lb,3))\n",
    "print(prevision(\"man\",Min,Mout,text_lb,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper amelioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_2 = SourceModule(\"\"\"\n",
    "\n",
    "\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "#include <curand.h>\n",
    "#include <math.h>\n",
    "\n",
    "\n",
    "__device__ void multiplication(float* A, float* B, float* C, int* dim){\n",
    "    \n",
    "    int lA = dim[0];\n",
    "    int cA = dim[1];\n",
    "    int lB = dim[2];\n",
    "    int cB = dim[3];\n",
    "    \n",
    "    if (cA!=lB){\n",
    "        return ;\n",
    "    }\n",
    "    \n",
    "    for(int i=0; i<lA ; i++){\n",
    "        for(int j=0; j<cB ; j++){\n",
    "            C[i*cB+j] = 0;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for(int i=0; i<lA ; i++){\n",
    "        for(int j=0; j<cB ; j++){\n",
    "            for(int k=0 ; k<cA ; k++){\n",
    "                C[i*cB+j] = C[i*cB+j] + A[i*cA+k]*B[k*cB+j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "__device__ void transpose(float* A, float* TA, int* dim){\n",
    "    \n",
    "    int lA = dim[0];\n",
    "    int cA = dim[1];\n",
    "      \n",
    "    for(int i=0; i<cA ; i++){\n",
    "        for(int j=0; j<lA ; j++){\n",
    "            TA[i*lA+j] = A[j*cA+i];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "__device__ void loop(float *Min, float *Mout, float alpha, int *Nwin, int wout, int V, int d, int N, int negative, int r){\n",
    "\n",
    "\n",
    "    /*Calcul des target_words (wout et negative sample)*/\n",
    "    int *target_words;\n",
    "    target_words = (int *)malloc(sizeof(int)*(negative+1));\n",
    "    for(int i=0; i<negative+1;i++){\n",
    "        if (i==0){\n",
    "            target_words[i] = wout;\n",
    "        } else {\n",
    "            r = (1664525*r+1013904223) % 4294967296;\n",
    "            target_words[i] = r%V;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /*label*/\n",
    "    float *label;\n",
    "    label = (float *)malloc(sizeof(float)*(2*N*(negative+1)));\n",
    "    for (int i=0; i<2*N ; i++){\n",
    "        for (int j=0 ; j<negative+1;j++){\n",
    "            if (j==0){\n",
    "                label[i*(negative+1)+j] = 1;\n",
    "            } else {\n",
    "                label[i*(negative+1)+j] = 0;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /*sub_Min*/\n",
    "    float* sub_Min;\n",
    "    sub_Min = (float *)malloc(sizeof(float)*2*N*d);\n",
    "    for (int i=0; i<2*N ; i++){\n",
    "        for (int j=0 ; j<d ; j++){\n",
    "            sub_Min[i*d+j] = Min[Nwin[i]*d+j];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /*sub_Mout*/\n",
    "    float* sub_Mout;\n",
    "    sub_Mout = (float *)malloc(sizeof(float)*(negative+1)*d);\n",
    "    for (int i=0; i<negative+1 ; i++){\n",
    "        for (int j=0 ; j<d ; j++){\n",
    "            sub_Mout[i*d+j] = Mout[target_words[i]*d+j];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /*sub_Mout_transpose*/\n",
    "    float* sub_Mout_transpose;\n",
    "    sub_Mout_transpose = (float *)malloc(sizeof(float)*d*(negative+1));\n",
    "    int dim_sub_Mout[2];\n",
    "    dim_sub_Mout[0] = negative+1;\n",
    "    dim_sub_Mout[1] = d;\n",
    "    transpose(sub_Mout, sub_Mout_transpose, dim_sub_Mout);\n",
    "    \n",
    "    /*INN*/\n",
    "    float* INN;\n",
    "    INN = (float *)malloc(sizeof(float)*2*N*(negative+1));\n",
    "    int dim[4];\n",
    "    dim[0] = 2*N;\n",
    "    dim[1] = d;\n",
    "    dim[2] = d;\n",
    "    dim[3] = negative+1;\n",
    "    multiplication(sub_Min, sub_Mout_transpose, INN, dim);\n",
    "    \n",
    "    /*ERR*/\n",
    "    float* ERR;\n",
    "    ERR = (float *)malloc(sizeof(float)*2*N*(negative+1));\n",
    "    for (int i=0; i<2*N ; i++){\n",
    "        for (int j=0 ; j<negative+1;j++){\n",
    "            ERR[i*(negative+1)+j] = label[i*(negative+1)+j] - (1/(1+exp(-1*INN[i*(negative+1)+j])));\n",
    "        }\n",
    "    }  \n",
    "    \n",
    "    /*TEMP*/\n",
    "    float* TEMP;\n",
    "    TEMP = (float *)malloc(sizeof(float)*2*N*d);\n",
    "    dim[0] = 2*N;\n",
    "    dim[1] = negative+1;\n",
    "    dim[2] = negative+1;\n",
    "    dim[3] = d;\n",
    "    multiplication(ERR, sub_Mout, TEMP, dim);\n",
    "    \n",
    "    /*MAJ Mout*/\n",
    "    float* ERR_transpose_alpha;\n",
    "    ERR_transpose_alpha = (float *)malloc(sizeof(float)*(negative+1)*2*N);\n",
    "    for (int i=0; i<negative+1 ; i++){\n",
    "        for (int j=0 ; j<2*N;j++){\n",
    "            ERR_transpose_alpha[i*2*N+j] = alpha*ERR[j*(negative+1)+i]; \n",
    "        }\n",
    "    }\n",
    "    float* MAJ_Mout;\n",
    "    MAJ_Mout = (float *)malloc(sizeof(float)*(negative+1)*d);\n",
    "    dim[0] = negative+1;\n",
    "    dim[1] = 2*N;\n",
    "    dim[2] = 2*N;\n",
    "    dim[3] = d;\n",
    "    multiplication(ERR_transpose_alpha, sub_Min, MAJ_Mout, dim);\n",
    "    for (int i=0; i<negative+1 ; i++){\n",
    "        for (int j=0 ; j<d;j++){\n",
    "            Mout[target_words[i]*d+j] = Mout[target_words[i]*d+j] + MAJ_Mout[i*d+j]; \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /*MAJ Min*/\n",
    "    for(int i = 0;i<2*N;i++){\n",
    "        for(int j = 0;j<d;j++){\n",
    "            Min[Nwin[i]*d+j] = Min[Nwin[i]*d+j] + alpha*TEMP[i*d+j];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    free(target_words);\n",
    "    free(label);\n",
    "    free(sub_Min);\n",
    "    free(sub_Mout);\n",
    "    free(sub_Mout_transpose);\n",
    "    free(INN);\n",
    "    free(ERR);\n",
    "    free(TEMP);\n",
    "    free(ERR_transpose_alpha);\n",
    "    free(MAJ_Mout);\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "__global__ void parallel(float *Min, float* Mout, int random, int* cst_int, float* cst_float, int* targets, int* contexts) {\n",
    "\n",
    "    /*constantes entières passés depuis le code python*/\n",
    "    int V = cst_int[0];\n",
    "    int d = cst_int[1];\n",
    "    int negative = cst_int[2];\n",
    "    int N = cst_int[3];\n",
    "\n",
    "    /*constante float passé depuis python*/\n",
    "    float alpha = cst_float[0];\n",
    "    \n",
    "    /*l'index correspond au thread et indique simplement le wout sur lequel on travaille*/\n",
    "    int index = threadIdx.x; \n",
    "    int wout = targets[index];\n",
    "    \n",
    "    /*Context de wout*/\n",
    "    int *Nwin;\n",
    "    Nwin = (int *)malloc(sizeof(int)*N*2);\n",
    "    for (int i=0;i<2*N;i++){\n",
    "        Nwin[i] = contexts[index*2*N+i];\n",
    "    }\n",
    "    \n",
    "    random = random+index;\n",
    "    \n",
    "    loop(Min,Mout,alpha,Nwin,wout,V,d,N,negative,random);\n",
    "   \n",
    "    free(Nwin);\n",
    "    \n",
    "}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_2 = mod_2.get_function(\"parallel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps d'execution de 10 epochs\n",
      "32.57986545562744\n",
      "[[ 7.4137259e-01  4.7478074e-01  3.3267787e-01 ...  9.5184082e-01\n",
      "   1.8321779e-01  9.3742126e-01]\n",
      " [ 7.9007107e-01  7.5255549e-01  4.6027899e-01 ... -1.2118397e-01\n",
      "   1.7460804e-01  6.6338271e-01]\n",
      " [ 4.6469308e-02  4.8930153e-01  7.8742313e-01 ...  8.3109987e-01\n",
      "   1.0589902e-01  8.2291704e-01]\n",
      " ...\n",
      " [-1.5304189e-02  6.4523506e-01  4.2101943e-01 ...  5.2118313e-01\n",
      "   5.5230379e-01  1.0219754e-01]\n",
      " [ 7.5730002e-01  7.7641970e-01  9.0988910e-01 ...  1.4865802e-01\n",
      "   7.2649352e-02  6.9265908e-01]\n",
      " [ 8.3669215e-02  6.8336993e-01  2.5119770e-01 ...  2.3244886e-01\n",
      "   5.7150173e-04  1.8801883e-01]]\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "Min, Mout = train_word2vec_parallel(X=X_hot, params=params, func=func_2)\n",
    "dt=time.time()-t0\n",
    "print(\"temps d'execution de %d epochs\" % params[\"n_epochs\"])\n",
    "print(dt)\n",
    "print(Min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one' 'was' 'he']\n",
      "['that' 'had' 'not']\n",
      "['that' 'i' 'had']\n"
     ]
    }
   ],
   "source": [
    "print(prevision(\"jean\",Min,Mout,text_lb,3))\n",
    "print(prevision(\"valjean\",Min,Mout,text_lb,3))\n",
    "print(prevision(\"woman\",Min,Mout,text_lb,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lb.inverse_transform(np.array([0]*6126+[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
